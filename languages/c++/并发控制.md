## 如何设计并发

-   设计并发数据结构要考虑两点，一是确保访问 thread-safe，二是提高并发度
    -   thread-safe 基本要求如下
        -   数据结构的不变量（invariant）被一个线程破坏时，确保不被线程看到此状态
        -   提供操作完整的函数来避免数据结构接口中固有的 race condition
        -   注意数据结构出现异常时的行为，以确保不变量不被破坏
        -   限制锁的范围，避免可能的嵌套锁，最小化死锁的概率
    -   作为数据结构的设计者，要提高数据结构的并发度，可以从以下角度考虑
        -   部分操作能否在锁的范围外执行
        -   数据结构的不同部分是否被不同的 mutex 保护
        -   是否所有操作需要同级别的保护
        -   在不影响操作语义的前提下，能否对数据结构做简单的修改提高并发度
    -   总结为一点，即最小化线程对共享数据的轮流访问，最大化真实的并发量

[[languages/c++/并发控制#一些范例]]

## 线程

## join

在 C++ 并发编程中，`join()` 是 `std::thread` 类的一个成员函数。当你在主线程或其他线程上调用某个线程对象的 `join()` 函数时，调用线程会阻塞，等待被 `join()` 的线程执行完成。

具体来说，`join()` 函数的操作如下：

1.  阻塞调用线程：调用线程（通常是主线程或其他线程）会停止执行，等待被 `join()` 的线程完成。在此期间，调用线程不会进行任何操作，而是等待。
    
2.  同步线程执行：被 `join()` 的线程在执行完毕后，调用线程会继续执行。这意味着你可以确保在调用线程继续执行之前，被 `join()` 的线程已经完成了它的任务。这对于确保线程间的正确同步和避免竞争条件非常重要。
    
3.  释放线程资源：一旦被 `join()` 的线程执行完毕，相关的线程资源将被释放，以便操作系统可以回收并分配给其他线程。调用 `join()` 会确保在线程对象销毁之前，资源得到妥善释放。
    

请注意，在 C++11 及更高版本中，如果在线程对象销毁时仍然可以 `join()`，则会触发 `std::terminate()`。为了避免程序终止，你必须确保在销毁线程对象之前调用 `join()` 或者 `detach()`。`join()` 用于同步线程执行，而 `detach()` 会让线程独立运行，不再与线程对象关联。

```cpp
#include <iostream>
#include <thread>

void print_hello() {
  std::cout << "Hello from the spawned thread!" << std::endl;
}

int main() {
  // 创建一个新线程并执行 print_hello 函数
  std::thread my_thread(print_hello);

  // 在主线程中调用 join，等待新线程执行完毕
  my_thread.join();

  std::cout << "Hello from the main thread!" << std::endl;
  return 0;
}
```

在这个例子中，我们在主线程中调用 `my_thread.join()`，这会导致主线程阻塞，等待 `my_thread` 执行 `print_hello` 函数。一旦 `my_thread` 完成，主线程将继续执行，并输出 "Hello from the main thread!"。这样可以确保 "Hello from the spawned thread!" 总是在 "Hello from the main thread!" 之前输出。

### detach

分离线程会让线程在后台运行，一般将这种在后台运行的线程称为守护线程，守护线程与主线程无法直接交互，也不能被join,创建守护进程主要是为了长时间运行

### 为带参数的函数创建线程

1. 带参数的函数也能传给线程，但是默认参数会被忽略
```cpp
#include <thread>

void f(int i = 1) {}

int main() {
  std::thread t{f, 42};  // std::thread t{f} 则会出错，因为默认实参会被忽略
  t.join();
}
```
2. 参数的引用类型也会被忽略，因此需要使用std::ref
```cpp
#include <cassert>
#include <thread>

void f(int& i) { ++i; }

int main() {
  int i = 1;
  std::thread t{f, std::ref(i)};
  t.join();
  assert(i == 2);
}
```

### 如果对一个实例的 non-static 成员函数创建线程，第一个参数类型为成员函数指针，第二个参数类型为实例指针，后续参数为函数参数

```cpp
#include<iostream>
#include<thread>

using std::ostream;

class Point{
	int x;
	int y;
	public:
		Point(int _x, int _y):x(_x),y(_y){}
		void mid(const Point& pt){
			this->x = ( this->x + pt.x ) / 2;
			this->y = ( this->y + pt.y ) / 2;
		}
		friend ostream& operator<<(ostream&,const Point&);
};

ostream& operator<<(ostream& os, const Point& p){
	os<<p.x<<" "<<p.y<<std::endl;
	return os;
}

int main(){
	Point p1 = Point(1,3);
	Point p2 = Point(5,7);
	std::thread t1{&Point::mid,&p1,std::ref(p2)};
	t1.join();
	std::cout<<p1<<std::endl;
	return 0;
}
```

### 如果要为参数是 move-only 类型的函数创建线程，则需要使用 [std::move](https://en.cppreference.com/w/cpp/utility/move) 传入参数

```cpp
#include <iostream>
#include <thread>
#include <utility>

void f(std::unique_ptr<int> p) { std::cout << *p; }

int main() {
  std::unique_ptr<int> p(new int(42));
  std::thread t{f, std::move(p)};
  t.join();
}
```

### 转移线程所有权

std::thread 是move-only的，并且不可以转让所有权到joinable的线程


### 一次性join大量线程

```cpp
#include <algorithm>
#include <thread>
#include <vector>

int main() {
  std::vector<std::thread> v;
  for (int i = 0; i < 10; ++i) {
    v.emplace_back([] {});
  }
  std::for_each(std::begin(v), std::end(v), std::mem_fn(&std::thread::join));
}
```

***std::thread可以作为函数的返回值也可以作为函数的参数***


## 线程管理

### 运行时决定线程数量

`std::thread::hardware_concurrency()`可以用来返回能并发在一个程序中的线程数量

```cpp
#include <thread>
#include <numeric>
#include <algorithm>
#include <functional>
#include <vector>
#include <iostream>

template<typename Iterator,typename T>
struct accumulate_block
{
    void operator()(Iterator first,Iterator last,T& result)
    {
        result=std::accumulate(first,last,result);
    }
};

template<typename Iterator,typename T>
T parallel_accumulate(Iterator first,Iterator last,T init)
{
    unsigned long const length=std::distance(first,last);

    if(!length)
        return init;

    unsigned long const min_per_thread=25;
    unsigned long const max_threads=
        (length+min_per_thread-1)/min_per_thread;

    unsigned long const hardware_threads=
        std::thread::hardware_concurrency();

    unsigned long const num_threads=
        std::min(hardware_threads!=0?hardware_threads:2,max_threads);

    unsigned long const block_size=length/num_threads;

    std::vector<T> results(num_threads);
    std::vector<std::thread>  threads(num_threads-1);

    Iterator block_start=first;
    for(unsigned long i=0;i<(num_threads-1);++i)
    {
        Iterator block_end=block_start;
        std::advance(block_end,block_size);
        threads[i]=std::thread(
            accumulate_block<Iterator,T>(),
            block_start,block_end,std::ref(results[i]));
        block_start=block_end;
    }
    accumulate_block<Iterator,T>()(block_start,last,results[num_threads-1]);
    
    std::for_each(threads.begin(),threads.end(),
        std::mem_fn(&std::thread::join));

    return std::accumulate(results.begin(),results.end(),init);
}

int main()
{
    std::vector<int> vi;
    for(int i=0;i<10;++i)
    {
        vi.push_back(10);
    }
    int sum=parallel_accumulate(vi.begin(),vi.end(),5);
    std::cout<<"sum="<<sum<<std::endl;
}
```


其实这里的parallel_accumulate等价于std::reduce

## 线程间共享数据

### std::lock_guard

`std::lock_guard`是C++11标准库中提供的一个用于管理互斥量（mutex）的便捷RAII（Resource Acquisition Is Initialization，资源获取即初始化）包装器。`std::lock_guard`的主要作用是简化互斥量的锁定和解锁操作，以避免死锁、遗漏解锁等问题。当你创建一个`std::lock_guard`对象时，它会自动锁定关联的互斥量；当`std::lock_guard`对象离开作用域时，它会自动解锁关联的互斥量。

下面是一个`std::lock_guard`的简单示例：
```cpp
#include <iostream>
#include <mutex>
#include <thread>

std::mutex mtx;
int shared_data = 0;

void increase_data() {
    for (int i = 0; i < 1000; ++i) {
        std::lock_guard<std::mutex> lock(mtx);
        ++shared_data;
        // mtx会在lock_guard对象离开作用域时自动解锁
    }
}

int main() {
    std::thread t1(increase_data);
    std::thread t2(increase_data);

    t1.join();
    t2.join();

    std::cout << "Shared data: " << shared_data << std::endl;

    return 0;
}
```

在这个示例中，我们有一个互斥量`mtx`和一个全局整数变量`shared_data`。我们创建了两个线程`t1`和`t2`，它们都调用`increase_data`函数。这个函数的目的是增加`shared_data`的值。

在`increase_data`函数中，我们使用`std::lock_guard`来自动锁定和解锁互斥量`mtx`。当`std::lock_guard`对象`lock`被创建时，它会自动锁定关联的互斥量。当`lock`对象离开作用域时，它会自动解锁关联的互斥量。这样，我们可以确保在访问和修改`shared_data`时只有一个线程，从而避免了数据竞争和其他并发问题。

`std::lock_guard`的优势在于它简化了互斥量的管理，通过RAII机制自动处理锁定和解锁。这意味着在发生异常或函数提前返回的情况下，你不需要担心是否已经正确解锁互斥量。`std::lock_guard`会确保互斥量在对象离开作用域时被正确解锁。

请注意，`std::lock_guard`不允许显式解锁或在运行时更改关联的互斥量。如果需要更灵活的互斥量管理，可以使用`std::unique_lock`，它提供了更多的功能和选项。然而，在许多情况下，`std::lock_guard`提供了足够的功能，并且在性能和简洁性方面具有优势。

相比`std::unique_lock`，`std::lock_guard`的性能通常更高，因为它不需要处理锁的懒惰（延迟）初始化、可重入锁、条件变量或其他更复杂的同步操作。`std::lock_guard`仅仅关注锁定和解锁互斥量，这使得它在实现上更简单，执行速度更快。

在以下情况下，使用`std::lock_guard`是一个好主意：

1.  当你只需要锁定和解锁一个互斥量时。
2.  当你不需要显式解锁或在运行时更改关联的互斥量时。
3.  当你不需要支持条件变量、可重入锁或其他高级同步功能时。

总之，`std::lock_guard`是一个简单而高效的互斥量管理工具，适用于大多数简单的同步需求。在涉及多线程编程时，`std::lock_guard`可以自动处理互斥量的锁定和解锁，从而避免了许多常见的并发问题。如果需要更高级的同步功能，可以考虑使用`std::unique_lock`。


### std::scoped_lock

`std::scoped_lock`是C++17标准库引入的一个RAII（Resource Acquisition Is Initialization）类型，用于同时锁定多个互斥量。`std::scoped_lock`的目的是避免死锁，确保在多线程环境下对多个互斥量的正确和安全管理。与`std::lock_guard`类似，`std::scoped_lock`在创建时锁定互斥量，并在离开作用域时自动解锁。不同之处在于，`std::scoped_lock`支持锁定多个互斥量。

`std::scoped_lock`的使用场景是需要同时锁定多个互斥量以确保原子操作或保护多个共享资源的情况。`std::scoped_lock`通过按照一个特定顺序锁定多个互斥量，从而避免死锁。在锁定过程中，如果锁定失败，`std::scoped_lock`会自动回滚已锁定的互斥量并重试，直到所有互斥量都被成功锁定。

下面是一个`std::scoped_lock`的示例：

```cpp
#include <iostream>
#include <mutex>
#include <thread>

std::mutex mtx1;
std::mutex mtx2;
int shared_data1 = 0;
int shared_data2 = 0;

void modify_data() {
    for (int i = 0; i < 1000; ++i) {
        std::scoped_lock lock(mtx1, mtx2);
        ++shared_data1;
        ++shared_data2;
        // mtx1和mtx2会在scoped_lock对象离开作用域时自动解锁
    }
}

int main() {
    std::thread t1(modify_data);
    std::thread t2(modify_data);

    t1.join();
    t2.join();

    std::cout << "Shared data 1: " << shared_data1 << std::endl;
    std::cout << "Shared data 2: " << shared_data2 << std::endl;

    return 0;
}
```

在这个示例中，我们有两个互斥量`mtx1`和`mtx2`，以及两个全局整数变量`shared_data1`和`shared_data2`。我们创建了两个线程`t1`和`t2`，它们都调用`modify_data`函数。这个函数的目的是同时增加`shared_data1`和`shared_data2`的值。

在`modify_data`函数中，我们使用`std::scoped_lock`同时锁定互斥量`mtx1`和`mtx2`。当`std::scoped_lock`对象`lock`被创建时，它会自动锁定关联的互斥量。当`lock`对象离开作用域时，它会自动解锁关联的互斥量。这样，我们可以确保在访问和修改`shared_data1`和`shared_data2`时只有一个线程，从而避免了数据竞争和其他并发问题。

`std::scoped_lock`的优势在于它简化了多个互斥量的管理，通过RAII机制自动处理锁定和解锁。这意味着在发生异常或函数提前返回的情况下，你不需要担心是否已经正确解锁互斥量。`std::scoped_lock`会确保互斥量在对象离开作用域时被正确解锁。

另外，`std::scoped_lock`提供了一个简单的方法来避免死锁。当多个线程需要按照不同的顺序锁定多个互斥量时，可能会导致死锁。`std::scoped_lock`解决这个问题的方法是总是按照相同的顺序锁定互斥量，从而消除了死锁的可能性。

请注意，`std::scoped_lock`不允许显式解锁或在运行时更改关联的互斥量。此外，`std::scoped_lock`不支持条件变量或其他高级同步功能。如果需要这些功能，可以考虑使用`std::unique_lock`。

总之，`std::scoped_lock`是一个简单而高效的多互斥量管理工具。它通过RAII机制自动处理锁定和解锁多个互斥量，确保了多线程编程中多个互斥量的正确和安全使用。在涉及多个共享资源的场景下，`std::scoped_lock`可以提供简洁且安全的同步解决方案。

在这个示例中，`mtx1`和`mtx2`并不直接知道它们对应的是`shared_data1`和`shared_data2`。实际上，互斥量和共享数据之间的关联是通过程序员的约定和代码组织来实现的。换句话说，我们通过编写程序来确保在访问和修改`shared_data1`时使用`mtx1`，在访问和修改`shared_data2`时使用`mtx2`。

这种关联取决于你如何编写和组织代码。在这个示例中，我们通过使用`std::scoped_lock`同时锁定`mtx1`和`mtx2`，确保了在访问和修改`shared_data1`和`shared_data2`时只有一个线程。这是因为在`modify_data`函数中，我们使用`std::scoped_lock lock(mtx1, mtx2)`创建了一个`scoped_lock`对象，它会自动锁定`mtx1`和`mtx2`。接下来，我们在`lock`对象的作用域内访问和修改`shared_data1`和`shared_data2`。当`lock`对象离开作用域时，它会自动解锁`mtx1`和`mtx2`。

因此，关键在于如何编写代码以确保在访问和修改共享数据时使用正确的互斥量。你需要确保在访问或修改共享数据之前锁定与之关联的互斥量，并在完成操作后解锁。通过这种方式，你可以确保多线程环境下共享数据的正确和安全访问。


## 让const线程安全

```cpp
class Polynomial {
public:
    using RootsType = std::vector<double>;
    
    RootsType roots() const
    {
        std::lock_guard<std::mutex> g(m);       //锁定互斥量
        
        if (!rootsAreValid) {                   //如果缓存无效
            …                                   //计算/存储根值
            rootsAreValid = true;
        }
        
        return rootsVals;
    }                                           //解锁互斥量
    
private:
    mutable std::mutex m;
    mutable bool rootsAreValid { false };
    mutable RootsType rootsVals {};
};
```

`std::mutex m`被声明为`mutable`，因为锁定和解锁它的都是non-`const`成员函数。在`roots`（`const`成员函数）中，`m`却被视为`const`对象。

值得注意的是，因为`std::mutex`是一种只可移动类型（_move-only type_，一种可以移动但不能复制的类型），所以将`m`添加进`Polynomial`中的副作用是使`Polynomial`失去了被复制的能力。不过，它仍然可以移动。


在轻量级的操作中，比如说计数，这时就没必要使用开销巨大的mutex了，可以使用atomic

```cpp
class Point {                                   //2D点
public:
    …
    double distanceFromOrigin() const noexcept  //noexcept的使用
    {                                           //参考条款14
        ++callCount;                            //atomic的递增
        
        return std::sqrt((x * x) + (y * y));
    }

private:
    mutable std::atomic<unsigned> callCount{ 0 };
    double x, y;
};
```

与`std::mutex`一样，`std::atomic`是只可移动类型，所以在`Point`中存在`callCount`就意味着`Point`也是只可移动的

对于需要同步的是单个的变量或者内存位置，使用`std::atomic`就足够了。不过，一旦你需要对两个以上的变量或内存位置作为一个单元来操作的话，就应该使用互斥量





## CPU亲和性

-   一种性能优化的方法是，将线程绑定到一个指定的 CPU core 上运行，以避免多核 CPU 上下文切换和 cache miss 的开销，Linux 中实现如下

```cpp
#include <pthread.h>
#include <sched.h>
#include <string.h>

#include <iostream>
#include <thread>

void affinity_cpu(std::thread::native_handle_type t, int cpu_id) {
  cpu_set_t cpu_set;
  CPU_ZERO(&cpu_set);
  CPU_SET(cpu_id, &cpu_set);
  int res = pthread_setaffinity_np(t, sizeof(cpu_set), &cpu_set);
  if (res != 0) {
    errno = res;
    std::cout << "fail to affinity" << strerror(errno) << std::endl;
  }
}

void f() {}

int main() {
  int cpu_id = 0;
  std::thread t{f};
  affinity_cpu(t.native_handle(), cpu_id);
  t.join();
}
```

## 一个线程安全的stack
```cpp
#include <exception>
#include <memory>
#include <mutex>
#include <stack>
#include <utility>

struct EmptyStack : std::exception {
  const char* what() const noexcept { return "empty stack!"; }
};

template <typename T>
class ConcurrentStack {
 public:
  ConcurrentStack() = default;

  ConcurrentStack(const ConcurrentStack& rhs) {
    std::lock_guard<std::mutex> l(rhs.m_);
    s_ = rhs.s_;
  }

  ConcurrentStack& operator=(const ConcurrentStack&) = delete;

  void push(T x) {
    std::lock_guard<std::mutex> l(m_);
    s_.push(std::move(x));
  }

  bool empty() const {
    std::lock_guard<std::mutex> l(m_);
    return s_.empty();
  }

  std::shared_ptr<T> pop() {
    std::lock_guard<std::mutex> l(m_);
    if (s_.empty()) {
      throw EmptyStack();
    }
    auto res = std::make_shared<T>(std::move(s_.top()));
    s_.pop();
    return res;
  }

  void pop(T& res) {
    std::lock_guard<std::mutex> l(m_);
    if (s_.empty()) {
      throw EmptyStack();
    }
    res = std::move(s_.top());
    s_.pop();
  }

 private:
  mutable std::mutex m_;
  std::stack<T> s_;
};
```

## 死锁


死锁的四个条件：互斥，占有且等待，不可抢占，循环等待

避免死锁的四个建议：
1. 每个线程最多只加一把锁，当然这种情况下也有可能因为线程之间相互等待而造成死锁
2. 持有锁时不要调用用户代码
3. 按固定顺序获取锁，如果可以的话最好尝试用`std::lock`同时获取
4. 使用层级锁，如果一个锁被低层持有，就不允许在高层加锁

```cpp
#include <iostream>
#include <mutex>
#include <stdexcept>

class HierarchicalMutex {
 public:
  explicit HierarchicalMutex(int hierarchy_value)
      : cur_hierarchy_(hierarchy_value), prev_hierarchy_(0) {}

  void lock() {
    validate_hierarchy();  // 层级错误则抛异常
    m_.lock();
    update_hierarchy();
  }

  bool try_lock() {
    validate_hierarchy();
    if (!m_.try_lock()) {
      return false;
    }
    update_hierarchy();
    return true;
  }

  void unlock() {
    if (thread_hierarchy_ != cur_hierarchy_) {
      throw std::logic_error("mutex hierarchy violated");
    }
    thread_hierarchy_ = prev_hierarchy_;  // 恢复前一线程的层级值
    m_.unlock();
  }

 private:
  void validate_hierarchy() {
    if (thread_hierarchy_ <= cur_hierarchy_) {
      throw std::logic_error("mutex hierarchy violated");
    }
  }

  void update_hierarchy() {
    // 先存储当前线程的层级值（用于解锁时恢复）
    prev_hierarchy_ = thread_hierarchy_;
    // 再把其设为锁的层级值
    thread_hierarchy_ = cur_hierarchy_;
  }

 private:
  std::mutex m_;
  const int cur_hierarchy_;
  int prev_hierarchy_;
  static thread_local int thread_hierarchy_;  // 所在线程的层级值
};

// static thread_local 表示存活于一个线程周期
thread_local int HierarchicalMutex::thread_hierarchy_(INT_MAX);

HierarchicalMutex high(10000);
HierarchicalMutex mid(6000);
HierarchicalMutex low(5000);

void lf() {  // 最低层函数
  std::lock_guard<HierarchicalMutex> l(low);
  // 调用 low.lock()，thread_hierarchy_ 为 INT_MAX，
  // cur_hierarchy_ 为 5000，thread_hierarchy_ > cur_hierarchy_，
  // 通过检查，上锁，prev_hierarchy_ 更新为 INT_MAX，
  // thread_hierarchy_ 更新为 5000
}  // 调用 low.unlock()，thread_hierarchy_ == cur_hierarchy_，
// 通过检查，thread_hierarchy_ 恢复为 prev_hierarchy_ 保存的 INT_MAX，解锁

void hf() {
  std::lock_guard<HierarchicalMutex> l(high);  // high.cur_hierarchy_ 为 10000
  // thread_hierarchy_ 为 10000，可以调用低层函数
  lf();  // thread_hierarchy_ 从 10000 更新为 5000
  //  thread_hierarchy_ 恢复为 10000
}  //  thread_hierarchy_ 恢复为 INT_MAX

void mf() {
  std::lock_guard<HierarchicalMutex> l(mid);  // thread_hierarchy_ 为 6000
  hf();  // thread_hierarchy_ < high.cur_hierarchy_，违反了层级结构，抛异常
}

int main() {
  lf();
  hf();
  try {
    mf();
  } catch (std::logic_error& ex) {
    std::cout << ex.what();
  }
}
```


## std::shared_mutex

`std::shared_mutex`是C++17标准库中引入的一种同步原语，用于保护共享数据结构，以便在多线程环境中安全地访问。`std::shared_mutex`允许多个线程以共享模式（读模式）同时访问受保护的数据，但在独占模式（写模式）下，只允许一个线程访问。这种读写锁的机制能够提高多线程程序的性能，因为读操作通常比写操作频繁得多。

```cpp
#include <iostream>
#include <shared_mutex>
#include <thread>
std::shared_mutex sm;
void write_data() {
    std::unique_lock<std::shared_mutex> lock(sm);
    // 对共享数据进行写操作
}
void read_data() {
    std::shared_lock<std::shared_mutex> lock(sm);
    // 对共享数据进行读操作
}
int main() {
    std::thread writer(&write_data);
    std::thread reader1(&read_data);
    std::thread reader2(&read_data);

    writer.join();
    reader1.join();
    reader2.join();

    return 0;
}
```

```cpp
class A {
 public:
  int read() const {
    std::shared_lock<std::shared_mutex> l(m_);
    return n_;
  }

  int write() {
    std::unique_lock<std::shared_mutex> l(m_);
    return ++n_;
  }

 private:
  mutable std::shared_mutex m_;
  int n_ = 0;
};
```

注意头文件要有mutex

## std::recursive_mutex

std::mutex是不可重入的，未释放之前再次上锁是未定义行为

std::recursive_mutex则是允许递归的，它可以在一个线程上多次获取锁，但在其他线程获取锁之前必须释放所有的锁

```cpp
#include <iostream>
#include <mutex>
#include <thread>
std::recursive_mutex rm;

void protected_function(int depth) {
    std::unique_lock<std::recursive_mutex> lock(rm);

    if (depth > 0) {
        // 对共享数据进行操作
        protected_function(depth - 1);
    }
}

int main() {
    std::thread t1(&protected_function, 5);
    std::thread t2(&protected_function, 5);

    t1.join();
    t2.join();

    return 0;
}
```

## 保证单次操作

```cpp
#include <memory>
#include <mutex>
#include <thread>

class A {
 public:
  void f() {}
};

std::shared_ptr<A> p;
std::once_flag flag;

void init() {
  std::call_once(flag, [&] { p.reset(new A); });
  p->f();
}

int main() {
  std::thread t1{init};
  std::thread t2{init};

  t1.join();
  t2.join();
}
```

## 条件变量

如何做到等待另一个线程完成某个事件后，再让当前线程执行任务呢？

可以使用条件变量

```cpp
#include <condition_variable>
#include <iostream>
#include <mutex>
#include <thread>

class A {
 public:
  void step1() {
    {
      std::lock_guard<std::mutex> l(m_);
      step1_done_ = true;
    }
    std::cout << 1;
    cv_.notify_one();
  }

  void step2() {
    std::unique_lock<std::mutex> l(m_);
    cv_.wait(l, [this] { return step1_done_; });
    step2_done_ = true;
    std::cout << 2;
    cv_.notify_one();
  }

  void step3() {
    std::unique_lock<std::mutex> l(m_);
    cv_.wait(l, [this] { return step2_done_; });
    std::cout << 3;
  }

 private:
  std::mutex m_;
  std::condition_variable cv_;
  bool step1_done_ = false;
  bool step2_done_ = false;
};

int main() {
  A a;
  std::thread t1(&A::step1, &a);
  std::thread t2(&A::step2, &a);
  std::thread t3(&A::step3, &a);
  t1.join();
  t2.join();
  t3.join();
}  // 123
```

### std::condition_variable::notify_one

如果有线程在等待`*this`,那么调用notify_one会随机唤醒某一个线程

```cpp
#include <iostream>
#include <condition_variable>
#include <thread>
#include <chrono>
using namespace std::chrono_literals;
 
std::condition_variable cv;
std::mutex cv_m;
int i = 0;
bool done = false;
 
void waits()
{
    std::unique_lock<std::mutex> lk(cv_m);
    std::cout << "Waiting... \n";
    cv.wait(lk, []{return i == 1;});
    std::cout << "...finished waiting; i == " << i << '\n';
    done = true;
}
 
void signals()
{
    std::this_thread::sleep_for(200ms);
    std::cout << "Notifying falsely...\n";
    cv.notify_one(); // waiting thread is notified with i == 0.
                     // cv.wait wakes up, checks i, and goes back to waiting
 
    std::unique_lock<std::mutex> lk(cv_m);
    i = 1;
    while (!done) 
    {
        std::cout << "Notifying true change...\n";
        lk.unlock();
        cv.notify_one(); // waiting thread is notified with i == 1, cv.wait returns
        std::this_thread::sleep_for(300ms);
        lk.lock();
    }
}
 
int main()
{
    std::thread t1(waits), t2(signals);
    t1.join(); 
    t2.join();
}
```

可能的输出
```cpp
Waiting... 

Notifying falsely...

Notifying true change...

...finished waiting; i == 1
```


## 竞争条件race condition

竞争条件（Race Condition）是指在多线程或多进程环境中，当多个线程或进程同时访问共享资源，并且至少有一个线程或进程试图修改该资源时，程序的行为取决于这些线程或进程的相对执行顺序。由于线程或进程的执行顺序是不可预测的，这可能导致不一致的状态和不可预测的结果。竞争条件通常会导致数据损坏、程序崩溃或其他未定义行为。

以下是一个简单的C++示例，演示了竞争条件：
```cpp
#include <iostream>
#include <thread>

int shared_counter = 0;

void increment() {
    for (int i = 0; i < 100000; ++i) {
        ++shared_counter;
    }
}

int main() {
    std::thread t1(increment);
    std::thread t2(increment);

    t1.join();
    t2.join();

    std::cout << "Shared counter: " << shared_counter << std::endl;

    return 0;
}
```

![[Pasted image 20230408092756.png]]

在这个示例中，两个线程`t1`和`t2`同时对共享资源`shared_counter`进行递增操作。由于没有同步机制来保护对`shared_counter`的访问，因此这个程序存在竞争条件。在这种情况下，程序的输出结果可能不是预期的200000，而是一个更小的数字。

要解决竞争条件问题，我们需要引入同步原语（如互斥锁、读写锁等）以确保在任何时刻只有一个线程或进程可以访问或修改共享资源。以下是一个修改后的示例，使用`std::mutex`来解决竞争条件：

```cpp
#include <iostream>
#include <mutex>
#include <thread>

int shared_counter = 0;
std::mutex mtx;

void increment() {
    for (int i = 0; i < 100000; ++i) {
        std::unique_lock<std::mutex> lock(mtx);
        ++shared_counter;
        lock.unlock();
    }
}

int main() {
    std::thread t1(increment);
    std::thread t2(increment);

    t1.join();
    t2.join();

    std::cout << "Shared counter: " << shared_counter << std::endl;

    return 0;
}
```


在这个修复后的示例中，我们使用`std::mutex`来保护对共享资源`shared_counter`的访问。当一个线程持有互斥锁时，其他线程必须等待锁释放才能访问共享资源。这确保了共享资源在任何时刻只被一个线程访问，从而消除了竞争条件。

在多线程编程中，避免和解决竞争条件是关键。在处理共享资源时，请务必使用适当的同步原语来确保数据的一致性和程序的正确性。同时，也要注意同步原语的性能开销和可能导致的性能下降。过度同步可能会导致线程在等待锁时浪费大量时间，从而降低整体性能。因此，在使用同步原语时，需要在确保数据一致性和程序正确性的同时，考虑性能优化。

以下是一些建议，可以帮助您在多线程编程中避免竞争条件并保持较好的性能：

1.  最小化共享数据：尽量将数据限制在单个线程内部，避免跨线程共享。这样可以减少同步的需求和复杂性。当必须共享数据时，请确保正确同步对共享数据的访问。
    
2.  精细化锁定：尽量使用细粒度的锁来保护共享资源，而不是用一个全局锁保护整个数据结构。这样可以减少线程之间的争用和等待时间。
    
3.  使用读写锁：当读操作远比写操作频繁时，使用读写锁（如`std::shared_mutex`）可能会提高性能，因为它允许多个线程同时进行读操作，而不会互相阻塞。
    
4.  避免锁定顺序死锁：如果多个线程需要锁定多个互斥锁，确保它们总是以相同的顺序锁定，以避免死锁。
    
5.  使用无锁数据结构：在某些情况下，可以使用无锁数据结构（如原子操作和无锁队列）来避免使用锁。无锁数据结构利用特定硬件指令确保操作的原子性，从而在多线程环境中实现安全的并发访问。但是，无锁编程通常更复杂，需要更深入的理解和更多的实践。
    
6.  限制线程数量：在多线程编程中，线程数量对性能有很大影响。合理地限制线程数量可以降低竞争条件发生的概率。通常，线程数量不应大于硬件支持的并行度（例如，CPU核心数）。
    
7.  优先考虑高级并行编程模型：有时，使用高级并行编程模型（如任务并行库、并行算法）可能比使用低级的线程和同步原语更加简单且高效。这些模型通常提供更好的抽象和更自动化的同步，以便您专注于实际的计算任务。
## 一个线程安全的queue实现

```cpp
#include <condition_variable>
#include <memory>
#include <mutex>
#include <queue>
#include <utility>

template <typename T>
class ConcurrentQueue {
 public:
  ConcurrentQueue() = default;

  ConcurrentQueue(const ConcurrentQueue& rhs) {
    std::lock_guard<std::mutex> l(rhs.m_);
    q_ = rhs.q_;
  }

  void push(T x) {
    auto data = std::make_shared<T>(std::move(x));
    std::lock_guard<std::mutex> l(m_);
    q_.push(data);
    cv_.notify_one();
  }

  void wait_and_pop(T& res) {
    std::unique_lock<std::mutex> l(m_);
    cv_.wait(l, [this] { return !q_.empty(); });
    res = std::move(*q_.front());
    q_.pop();
  }

  std::shared_ptr<T> wait_and_pop() {
    std::unique_lock<std::mutex> l(m_);
    cv_.wait(l, [this] { return !q_.empty(); });
    auto res = q_.front();
    q_.pop();
    return res;
  }

  bool try_pop(T& res) {
    std::lock_guard<std::mutex> l(m_);
    if (q_.empty()) {
      return false;
    }
    res = std::move(*q_.front());
    q_.pop();
    return true;
  }

  std::shared_ptr<T> try_pop() {
    std::lock_guard<std::mutex> l(m_);
    if (q_.empty()) {
      return nullptr;
    }
    auto res = q_.front();
    q_.pop();
    return res;
  }

  bool empty() const {
    std::lock_guard<std::mutex> l(m_);
    return q_.empty();
  }

 private:
  mutable std::mutex m_;
  std::condition_variable cv_;
  std::queue<std::shared_ptr<T>> q_;
};
```

这个程序定义了一个名为`ConcurrentQueue`的线程安全队列模板类。这个类使用互斥锁（`std::mutex`）和条件变量（`std::condition_variable`）来保护对底层队列（`std::queue<std::shared_ptr<T>>`）的并发访问。根据程序的实现，它是线程安全的。以下是关键部分的分析：

1.  构造函数：默认构造函数什么都不做，是线程安全的。复制构造函数用`std::lock_guard`锁定源队列的互斥锁，确保在复制期间没有其他线程修改源队列。
    
2.  `push`函数：使用`std::lock_guard`锁定互斥锁，确保在将元素入队时队列不被其他线程访问。之后使用`notify_one`唤醒可能正在等待队列中元素的线程。
    
3.  `wait_and_pop`函数：使用`std::unique_lock`锁定互斥锁。使用条件变量`cv_`等待队列变为非空。当队列非空时，将元素弹出并返回。因为在弹出元素期间锁定了互斥锁，所以这个过程是线程安全的。
    
4.  `try_pop`函数：使用`std::lock_guard`锁定互斥锁。检查队列是否为空，如果不为空，则弹出元素并返回。这个过程是线程安全的，因为在操作队列期间锁定了互斥锁。
    
5.  `empty`函数：使用`std::lock_guard`锁定互斥锁并检查队列是否为空。这个过程也是线程安全的，因为在访问队列期间锁定了互斥锁。
    

总之，这个`ConcurrentQueue`类的实现是线程安全的，因为它在访问底层队列时使用了适当的同步原语（`std::mutex`和`std::condition_variable`）来保护共享数据。这样确保了在任何时刻只有一个线程可以访问或修改队列。

## 信号量 semaphore

acquire()就是p操作，对信号量减一，release()就是v操作，对信号量加一

```cpp
#include <iostream>
#include <semaphore>
#include <thread>

class A {
 public:
  void wait1() {
    sem_.acquire();
    std::cout << 1;
  }

  void wait2() {
    sem_.acquire();
    std::cout << 2;
  }

  void signal() { sem_.release(2); }

 private:
  std::counting_semaphore<2> sem_{0};  // 初始值 0，最大值 2
};

int main() {
  A a;
  std::thread t1(&A::wait1, &a);
  std::thread t2(&A::wait2, &a);
  std::thread t3(&A::signal, &a);
  t1.join();
  t2.join();
  t3.join();
}  // 12 or 21
```

`std::binary_semaphore`是最大值为1的信号量，下面两个语句等效

```cpp
std::counting_semaphore<1> sem_{0};
std::binary_semaphore sem_{0};
```

## 屏障barrier

`std::barrier`是C++20标准库中引入的一种同步原语，用于在并发编程中协调多个线程的执行。`std::barrier`允许一组线程在执行过程中相互等待，直到所有线程都到达某个同步点。当所有线程都到达同步点时，它们将同时释放并继续执行。`std::barrier`的目标是确保多个线程能够同时完成某一阶段的工作，然后继续执行下一阶段。这对于解决某些需要多个线程协作的问题（如分布式计算、并行算法等）非常有用。

头文件barrier,别忘了`-std=c++20`


std::barrier用一个值作为要等待的线程的数量来构造，调用`std::barrier::arrive_and_wait`会阻塞至所有线程完成任务

```cpp
#include <barrier>
#include <cassert>
#include <iostream>
#include <thread>

class A {
 public:
  void f() {
    std::barrier sync_point{3, [&]() noexcept { ++i_; }};
    for (auto& x : tasks_) {
      x = std::thread([&] {
        std::cout << 1;
        sync_point.arrive_and_wait();
        assert(i_ == 1);
        std::cout << 2;
        sync_point.arrive_and_wait();
        assert(i_ == 2);
        std::cout << 3;
      });
    }
    for (auto& x : tasks_) {
      x.join();  // 析构 barrier 前 join 所有使用了 barrier 的线程
    }  // 析构 barrier 时，线程再调用 barrier 的成员函数是 undefined behavior
  }

 private:
  std::thread tasks_[3] = {};
  int i_ = 0;
};

int main() {
  A a;
  a.f();
}

//output
//111222333
```

-   C++20 提供了 [std::latch](https://en.cppreference.com/w/cpp/thread/latch) 作为一次性屏障，它用一个值作为计数器的初始值来构造，[std::latch::count_down](https://en.cppreference.com/w/cpp/thread/latch/count_down) 将计数器减 1，[std::latch::wait](https://en.cppreference.com/w/cpp/thread/latch/wait) 将阻塞至计数器为 0，如果想让计数器减一并阻塞至为 0 则可以调用 [std::latch::arrive_and_wait](https://en.cppreference.com/w/cpp/thread/latch/arrive_and_wait)

```cpp
#include <iostream>
#include <latch>
#include <string>
#include <thread>

class A {
 public:
  void f() {
    for (auto& x : data_) {
      x.t = std::jthread([&] {
        x.s += x.s;
        done_.count_down();
      });
    }
    done_.wait();
    for (auto& x : data_) {
      std::cout << x.s << std::endl;
    }
  }

 private:
  struct {
    std::string s;
    std::jthread t;
  } data_[3] = {
      {"hello"},
      {"down"},
      {"demo"},
  };

  std::latch done_{3};
};

int main() {
  A a;
  a.f();
}
```

## std::future

std::thread只能运行函数，却不能取得函数的返回值，但std::future可以异步获取结果

```cpp
#include <future>
#include <iostream>

class A {
 public:
  int f(int i) { return i; }
};

int main() {
  A a;
  std::future<int> res = std::async(&A::f, &a, 1);
  std::cout << res.get();  // 1，阻塞至线程返回结果
}
```
注意std::future只可以get一次

## std::async

```cpp
namespace std {
enum class launch { // names for launch options passed to async
    async    = 0x1, // 运行新线程来执行任务
    deferred = 0x2  // 惰性求值，请求结果时才执行任务
};
}

// std::async 创建任务默认使用两者
std::async([] {}); // 等价于 std::async(std::launch::async | std::launch::deferred, [] {})
```

这段代码展示了C++11中`std::async`的用法。`std::async`是一个高级并发工具，用于异步地执行任务并返回一个`std::future`对象。`std::future`对象可以用于获取任务的结果。这段代码中，`std::launch`枚举定义了两种任务执行策略：

1.  `std::launch::async`：使用此选项，任务将在一个新线程上执行，允许并发执行。
2.  `std::launch::deferred`：使用此选项，任务将延迟执行，直到请求结果（调用`std::future::get()`）时才执行。任务在调用`get()`的线程上执行，不会创建新线程。

`std::async`的默认执行策略是`std::launch::async | std::launch::deferred`，这意味着实现可以自由选择使用`std::launch::async`或`std::launch::deferred`策略来执行任务。具体选择哪种策略取决于底层实现。

## std::packaged_task

`std::packaged_task`是C++11标准库中提供的一个用于异步任务执行的类模板。它可以将一个可调用对象（如函数、函数指针、lambda表达式等）包装成一个任务，任务的返回值将作为关联的`std::future`对象的结果。`std::packaged_task`的主要目的是将任务的执行与结果的获取解耦，使得任务可以在一个线程上执行，而结果可以在另一个线程上获取。这有助于实现更简洁、高效的多线程编程。

以下是`std::packaged_task`的一些关键概念：

1.  **任务创建**：通过提供一个可调用对象（如函数、lambda表达式等），创建一个`std::packaged_task`实例。例如：
```cpp
std::packaged_task<int()> task([] { return 42; });
```
 在这个例子中，我们创建了一个返回`42`的简单任务。
2.  **关联的`std::future`对象**：当创建`std::packaged_task`实例时，一个关联的`std::future`对象也会被创建。你可以通过`std::packaged_task::get_future()`成员函数获取这个`std::future`对象。这个`std::future`对象可以用于获取任务的结果，例如：
```cpp
std::future<int> result = task.get_future();
```
3. **任务执行**：要执行`std::packaged_task`中的任务，可以调用`std::packaged_task::operator()`。任务可以在当前线程上执行，也可以在另一个线程上执行。例如，你可以将任务移交给一个`std::thread`对象来并发执行
```cpp
std::thread t(std::move(task));
t.join();
```
在这个例子中，我们将任务移交给一个新的线程并执行。注意，我们使用`std::move()`将任务移动到新线程，因为`std::packaged_task`不可复制，但可以移动。
4.  **获取结果**：当任务执行完成后，你可以通过关联的`std::future`对象获取结果。为此，你可以调用`std::future::get()`函数。例如：
```cpp
int value = result.get(); // value将等于42
```

一个完整的示例

```cpp
#include <iostream>
#include <future>
#include <thread>

int main() {
    // 创建一个简单的任务
    std::packaged_task<int()> task([] { return 42; });

    // 获取关联的std::future对象
    std::future<int> result = task.get_future();

    // 将任务移交给一个新线程并执行
    std::thread t(std::move(task));
    t.join();

    // 获取任务的结果
    int value = result.get();
    std::cout << "The answer is: " << value << std::endl;

    return 0;
}
```

## std::promise

可以返回一个固定值，可以实现事件通知的效果，不同于std::condition_variable，std::promise只能通知一次，因此通常用来创建暂停状态的线程,std::promise只能关联一个std::future

```cpp
#include <chrono>
#include <future>
#include <iostream>

class A {
 public:
  void task() { std::cout << 1; }
  void wait_for_task() {
    ps_.get_future().wait();
    task();
  }
  void signal() { ps_.set_value(); }

 private:
  std::promise<void> ps_;
};

void task() { std::cout << 1; }

int main() {
  A a;
  std::thread t(&A::wait_for_task, &a);
  a.signal();
  t.join();
}
```

std::future 可以存储任务中的异常，std::promise需要手动存储异常，但是std::promise set_value()时的异常不会被存储到std::future中

```cpp
#include <future>
#include <iostream>
#include <stdexcept>

int main() {
  std::promise<int> ps;
  try {
    ps.set_value([] {
      throw std::logic_error("error");
      return 0;
    }());
  } catch (const std::exception& e) {
    std::cout << e.what() << std::endl;
  }
  ps.set_value(1);
  auto res = ps.get_future();
  std::cout << res.get();  // 1
}
```

在这段代码中，lambda表达式后面跟了一个圆括号，因为这表示要立即执行这个匿名函数。当你定义了一个lambda函数后，你需要调用它以执行其函数体。圆括号就是用于调用的。

在这个例子中，lambda函数会抛出一个`std::logic_error`异常。当你在`ps.set_value()`调用中立即执行这个lambda函数时，异常将在`try`块中被抛出并由`catch`块捕获。捕获到的异常将被输出到`std::cout`。

如果没有圆括号，代码将不能编译，因为`std::promise::set_value()`期望一个值而不是一个函数。

如果std::packaged_task和std::promise直到析构都没有设置值，std::future会抛出异常

## std::shared_future

-   [std::shared_future](https://en.cppreference.com/w/cpp/thread/shared_future) 可以多次获取结果，它可以通过 [std::future](https://en.cppreference.com/w/cpp/thread/future) 的右值构造。每一个 [std::shared_future](https://en.cppreference.com/w/cpp/thread/shared_future) 对象上返回的结果不同步，多线程访问 [std::shared_future](https://en.cppreference.com/w/cpp/thread/shared_future) 需要加锁防止 race condition，更好的方法是给每个线程拷贝一个 [std::shared_future](https://en.cppreference.com/w/cpp/thread/shared_future) 对象，这样就可以安全访问而无需加锁

```cpp
#include <future>

int main() {
  std::promise<void> ps;
  std::future<void> ft = ps.get_future();
  std::shared_future<void> sf(std::move(ft));
  // 或直接 std::shared_future<void> sf{ps.get_future()};
  ps.set_value();
  sf.get();
  sf.get();
}
```

不过也可以直接std::future::share()直接生成std::shared_future

```cpp
#include <future>

int main() {
  std::promise<void> ps;
  auto sf = ps.get_future().share();
  ps.set_value();
  sf.get();
  sf.get();
}
```

## std::atomic

-   原子类型不允许由另一个原子类型拷贝赋值，因为拷贝赋值调用了两个对象，破坏了操作的原子性。但可以用对应的内置类型赋值

```cpp
T operator=(T desired) noexcept;
T operator=(T desired) volatile noexcept;
atomic& operator=(const atomic&) = delete;
atomic& operator=(const atomic&) volatile = delete;
```

下面是std::atomic为了支持赋值提供的成员函数

```cpp
std::atomic<T>::store     // 替换当前值
std::atomic<T>::load      // 返回当前值
std::atomic<T>::exchange  // 替换值，并返回被替换前的值

// 与期望值比较，不等则将期望值设为原子值并返回 false
// 相等则将原子值设为目标值并返回 true
// 在缺少 CAS（compare-and-exchange）指令的机器上，weak 版本在相等时可能替换失败并返回 false
// 因此 weak 版本通常要求循环，而 strong 版本返回 false 就能确保不相等
std::atomic<T>::compare_exchange_weak
std::atomic<T>::compare_exchange_strong

std::atomic<T>::fetch_add        // 原子加法，返回相加前的值
std::atomic<T>::fetch_sub        // 原子减法，返回相减前的值
std::atomic<T>::fetch_and
std::atomic<T>::fetch_or
std::atomic<T>::fetch_xor
std::atomic<T>::operator++       // 前自增等价于 fetch_add(1) + 1
std::atomic<T>::operator++(int)  // 后自增等价于 fetch_add(1)
std::atomic<T>::operator--       // 前自减等价于 fetch_sub(1) - 1
std::atomic<T>::operator--(int)  // 后自减等价于 fetch_sub(1)
std::atomic<T>::operator+=       // fetch_add(x) + x
std::atomic<T>::operator-=       // fetch_sub(x) - x
std::atomic<T>::operator&=       // fetch_and(x) & x
std::atomic<T>::operator|=       // fetch_or(x) | x
std::atomic<T>::operator^=       // fetch_xor(x) ^ x
```

```cpp
std::atomic<int> i(5);
int j = i.fetch_and(3);  // 101 & 011 = 001，i 为 1，j 为 5
```


### std::atomic::flag

唯一保证lock-free的原子类型，只能用ATOMIC_FLAG_INIT初始化为false

```cpp
std::atomic_flag x = ATOMIC_FLAG_INIT;

x.clear(std::memory_order_release);  // 将状态设为 false
// 不能为读操作语义：memory_order_consume、memory_order_acquire、memory_order_acq_rel

bool y = x.test_and_set();  // 将状态设为 true 且返回之前的值
```

## 内存序

在C++中，内存序（memory order）是一种用于描述多线程程序中原子操作（atomic operations）之间的顺序关系的概念。内存序影响了原子操作之间的可见性和顺序一致性。在多线程环境下，不同线程可能对内存操作的顺序有不同的观察结果，这可能导致程序行为的不确定性。为了解决这个问题，C++提供了内存序来明确规定原子操作之间的顺序约束。

C++11引入了`<atomic>`库，定义了几种内存序：
1.  `std::memory_order_relaxed`：松散内存序，允许编译器和处理器对原子操作进行最大程度的优化。原子操作间没有顺序关系，只保证原子操作本身的原子性。在这种内存序下，不同线程可能观察到不同的操作顺序。这种内存序不允许循环依赖，常见于自增计数器
    
2.  `std::memory_order_acquire`：获取内存序，用于读操作。如果一个原子操作A使用`memory_order_acquire`，那么在A之后的所有读操作都不能重排序到A之前。它确保在获取锁之后的操作看到的都是最新的数据。
    
3.  `std::memory_order_release`：释放内存序，用于写操作。如果一个原子操作B使用`memory_order_release`，那么在B之前的所有写操作都不能重排序到B之后。它确保在释放锁之前的所有操作都已完成。
    
4.  `std::memory_order_acq_rel`：获取-释放内存序，结合了`memory_order_acquire`和`memory_order_release`的特性，适用于同时具有读和写操作的原子操作。它保证在获取锁后的操作看到的都是最新数据，并确保在释放锁前的所有操作都已完成。
    
5.  `std::memory_order_seq_cst`：顺序一致内存序，提供最严格的顺序保证。它要求所有线程都必须看到相同的操作顺序。这是默认的内存序，但通常性能较差，因为它限制了编译器和处理器的优化能力。
    

在原子操作中，可以通过指定内存序来实现不同程度的顺序保证。例如，使用`std::atomic`类型：
```cpp
std::atomic<int> x(0);

void thread1() {
    x.store(1, std::memory_order_release);
}

void thread2() {
    int a = x.load(std::memory_order_acquire);
    std::cout << a << std::endl;
}
```

在这个例子中，`thread1`以释放内存序将`x`设置为1，`thread2`以获取内存序加载`x`的值。这确保了`thread2`能看到`x`的最新值。

选择合适的内存序对于性能和正确性至关重要。过于严格的内存序可能导致不必要的性能开销，而过于宽松的内存序可能导致不正确的程序行为。因此，在设计多线程程序时，需要根据具体需求选择合适的内存序。

以下是一些建议，可以在选择内存序时参考：

1.  如果确保原子操作的原子性就足够了，而操作间的顺序关系并不重要，可以使用`std::memory_order_relaxed`。但要注意，这种情况下可能导致意料之外的结果，尤其是在高度竞争的场景下。
    
2.  当需要实现简单的锁或同步原语时，可以使用`std::memory_order_acquire`和`std::memory_order_release`。这两种内存序提供了适中的顺序保证，同时允许编译器和处理器进行一定程度的优化。
    
3.  如果原子操作涉及到同时读写数据，可以使用`std::memory_order_acq_rel`。
    
4.  当需要最严格的顺序保证时，可以使用`std::memory_order_seq_cst`。在许多情况下，顺序一致性可以帮助简化程序设计，避免复杂的同步问题。然而，这种内存序的性能开销较大，应谨慎使用。
    

在选择内存序时，务必充分了解程序的需求和原子操作的语义。使用适当的内存序可以在保证程序正确性的同时，提高多线程程序的性能。

```cpp
typedef enum memory_order {
  memory_order_relaxed,  // 无同步或顺序限制，只保证当前操作原子性
  memory_order_consume,  // 标记读操作，依赖于该值的读写不能重排到此操作前
  memory_order_acquire,  // 标记读操作，之后的读写不能重排到此操作前
  memory_order_release,  // 标记写操作，之前的读写不能重排到此操作后
  memory_order_acq_rel,  // 仅标记读改写操作，读操作相当于 acquire，写操作相当于 release
  memory_order_seq_cst   // sequential consistency：顺序一致性，不允许重排，所有原子操作的默认选项
} memory_order;
```

### release-consume ordering

-   对于标记为 memory_order_consume 原子变量 x 的读操作 R，当前线程中依赖于 x 的读写不允许重排到 R 之前，其他线程中对依赖于 x 的变量写操作对当前线程可见
-   如果线程 A 对一个原子变量x的写操作为 memory_order_release，线程 B 对同一原子变量的读操作为 memory_order_consume，带来的副作用是，线程 A 中所有 [dependency-ordered-before](https://en.cppreference.com/w/cpp/atomic/memory_order#Dependency-ordered_before) 该写操作的其他写操作（non-atomic和relaxed atomic），在线程 B 的其他依赖于该变量的读操作中可见
-   典型使用场景是访问很少进行写操作的数据结构（比如路由表），以及以指针为中介的 publisher-subscriber 场景，即生产者发布一个指针给消费者访问信息，但生产者写入内存的其他内容不需要对消费者可见，这个场景的一个例子是 RCU（Read-Copy Update）。该顺序的规范正在修订中，并且暂时不鼓励使用 memory_order_consume
```cpp
#include <atomic>
#include <cassert>
#include <thread>

std::atomic<int*> x;
int i;

void producer() {
  int* p = new int(42);
  i = 42;
  x.store(p, std::memory_order_release);
}

void consumer() {
  int* q;
  while (!(q = x.load(std::memory_order_consume))) {
  }
  assert(*q == 42);  // 一定不出错：*q 带有 x 的依赖
  assert(i == 42);   // 可能出错也可能不出错：i 不依赖于 x
}

int main() {
  std::thread t1(producer);
  std::thread t2(consumer);
  t1.join();
  t2.join();
}
```

### release-acquire ordering

-   对于标记为 memory_order_acquire 的读操作 R，当前线程的其他读写操作不允许重排到 R 之前，其他线程中在同一原子变量上所有的写操作在当前线程可见
-   如果线程 A 对一个原子变量的写操作 W 为 memory_order_release，线程 B 对同一原子变量的读操作为 memory_order_acquire，带来的副作用是，线程 A 中所有 [happens-before](https://en.cppreference.com/w/cpp/atomic/memory_order#Happens-before) W 的写操作（non-atomic 和 relaxed atomic）都在线程 B 中可见
-   典型使用场景是互斥锁，线程 A 的释放后被线程 B 获取，则 A 中释放锁之前发生在 critical section 的所有内容都在 B 中可见
```cpp
#include <atomic>
#include <cassert>
#include <thread>

std::atomic<int*> x;
int i;

void producer() {
  int* p = new int(42);
  i = 42;
  x.store(p, std::memory_order_release);
}

void consumer() {
  int* q;
  while (!(q = x.load(std::memory_order_acquire))) {
  }
  assert(*q == 42);  // 一定不出错
  assert(i == 42);   // 一定不出错
}

int main() {
  std::thread t1(producer);
  std::thread t2(consumer);
  t1.join();
  t2.join();
}
```
-   对于标记为 memory_order_release 的写操作 W，当前线程中的其他读写操作不允许重排到W之后，若其他线程 acquire 该原子变量，则当前线程所有 [happens-before](https://en.cppreference.com/w/cpp/atomic/memory_order#Happens-before) 的写操作在其他线程中可见，若其他线程 consume 该原子变量，则当前线程所有 [dependency-ordered-before](https://en.cppreference.com/w/cpp/atomic/memory_order#Dependency-ordered_before) W 的其他写操作在其他线程中可见
-   对于标记为 memory_order_acq_rel 的读改写（read-modify-write）操作，相当于写操作是 memory_order_release，读操作是 memory_order_acquire，当前线程的读写不允许重排到这个写操作之前或之后，其他线程中 release 该原子变量的写操作在修改前可见，并且此修改对其他 acquire 该原子变量的线程可见
-   [Release-Acquire ordering](https://en.cppreference.com/w/cpp/atomic/memory_order#Release-Acquire_ordering) 并不表示 total ordering

```cpp
#include <atomic>
#include <thread>

std::atomic<bool> x = false;
std::atomic<bool> y = false;
std::atomic<int> z = 0;

void write_x() {
  x.store(true,std::memory_order_release);  // 1 happens-before 3（由于 3 的循环）
}

void write_y() {
  y.store(true,std::memory_order_release);  // 2 happens-before 5（由于 5 的循环）
}

void read_x_then_y() {
  while (!x.load(std::memory_order_acquire)) {  // 3 happens-before 4
  }
  if (y.load(std::memory_order_acquire)) {  // 4
    ++z;
  }
}

void read_y_then_x() {
  while (!y.load(std::memory_order_acquire)) {  // 5 happens-before 6
  }
  if (x.load(std::memory_order_acquire)) {  // 6
    ++z;
  }
}

int main() {
  std::thread t1(write_x);
  std::thread t2(write_y);
  std::thread t3(read_x_then_y);
  std::thread t4(read_y_then_x);
  t1.join();
  t2.join();
  t3.join();
  t4.join();
  // z 可能为 0，134 y 为 false，256 x 为 false，但 12 之间没有关系
}
```

1. 为了让两个写操作有序，可以将其放在一个线程里
```cpp
#include <atomic>
#include <cassert>
#include <thread>

std::atomic<bool> x = false;
std::atomic<bool> y = false;
std::atomic<int> z = 0;

void write_x_then_y() {
  x.store(true, std::memory_order_relaxed);  // 1 happens-before 2
  y.store(true, std::memory_order_release);  // 2 happens-before 3（由于 3 的循环）
}

void read_y_then_x() {
  while (!y.load(std::memory_order_acquire)) {  // 3 happens-before 4
  }
  if (x.load(std::memory_order_relaxed)) {  // 4
    ++z;
  }
}

int main() {
  std::thread t1(write_x_then_y);
  std::thread t2(read_y_then_x);
  t1.join();
  t2.join();
  assert(z.load() != 0);  // 顺序一定为 1234，z 一定不为 0
}
```

2. 内存序的使用示范

```cpp
#include <atomic>
#include <cassert>

std::atomic<int> x = 0;
std::atomic<int> v[2];

void f() {
  v[0].store(1, std::memory_order_relaxed);
  v[1].store(2, std::memory_order_relaxed);
  x.store(1, std::memory_order_release);  // 1 happens-before 2（由于 2 的循环）
}

void g() {
  int i = 1;
  while (!x.compare_exchange_strong(
      i, 2,
      std::memory_order_acq_rel)) {  // 2 happens-before 3（由于 3 的循环）
    // x 为 1 时，将 x 替换为 2，返回 true
    // x 为 0 时，将 i 替换为 x，返回 false
    i = 1;  // 返回 false 时，x 未被替换，i 被替换为 0，因此将 i 重新设为 1
  }
}

void h() {
  while (x.load(std::memory_order_acquire) < 2) {  // 3
  }
  assert(v[0].load(std::memory_order_relaxed) == 1);
  assert(v[1].load(std::memory_order_relaxed) == 2);
}
```

这段代码展示了一个使用`std::atomic`和内存序的多线程同步示例。代码中定义了三个原子整数变量`x`和数组`v`，分别在不同的函数`f()`、`g()`和`h()`中进行操作。这些函数预期在不同的线程中执行。

1.  函数`f()`首先将`v[0]`和`v[1]`设置为1和2（使用松散内存序）。接下来，将`x`设置为1（使用释放内存序），表示`f()`已完成。
    
2.  函数`g()`使用`compare_exchange_strong`操作来修改`x`的值。当`x`为1时，`x`被替换为2，并返回`true`。当`x`为0时，`i`被替换为`x`的值，返回`false`。循环直到`x`成功替换为2。此操作使用获取-释放内存序，确保`x`的值在函数`f()`和`g()`之间正确传递。
    
3.  函数`h()`中，使用带有获取内存序的`x.load()`操作检查`x`的值。当`x`的值小于2时，循环继续。一旦`x`的值变为2，循环结束，表示`f()`和`g()`均已完成。然后，使用`assert`语句检查`v[0]`和`v[1]`的值是否为1和2，验证`f()`函数的操作已经完成。
    

这段代码展示了如何使用`std::atomic`和内存序在多线程程序中进行同步。通过`memory_order_release`、`memory_order_acq_rel`和`memory_order_acquire`，我们确保了`f()`、`g()`和`h()`之间的正确执行顺序，以及`v`数组值的正确可见性。

### sequentially-consistent ordering

-   memory_order_seq_cst 是所有原子操作的默认选项，可以省略不写。对于标记为 memory_order_seq_cst 的操作，读操作相当于 memory_order_acquire，写操作相当于 memory_order_release，读改写操作相当于 memory_order_acq_rel，此外还附加一个单独的 total ordering，即所有线程对同一操作看到的顺序也是相同的。这是最简单直观的顺序，但由于要求全局的线程同步，因此也是开销最大的

```cpp
#include <atomic>
#include <cassert>
#include <thread>

std::atomic<bool> x = false;
std::atomic<bool> y = false;
std::atomic<int> z = 0;

// 要么 1 happens-before 2，要么 2 happens-before 1
void write_x() {
  x.store(true);  // 1 happens-before 3（由于 3 的循环）
}

void write_y() {
  y.store(true);  // 2 happens-before 5（由于 5 的循环）
}

void read_x_then_y() {
  while (!x.load()) {  // 3 happens-before 4
  }
  if (y.load()) {  // 4 为 false 则 1 happens-before 2
    ++z;
  }
}

void read_y_then_x() {
  while (!y.load()) {  // 5 happens-before 6
  }
  if (x.load()) {  // 6 如果返回 false 则一定是 2 happens-before 1
    ++z;
  }
}

int main() {
  std::thread t1(write_x);
  std::thread t2(write_y);
  std::thread t3(read_x_then_y);
  std::thread t4(read_y_then_x);
  t1.join();
  t2.join();
  t3.join();
  t4.join();
  assert(z.load() != 0);  // z 一定不为 0
  // z 可能为 1 或 2，12 之间必定存在 happens-before 关系
}
```


## 自旋锁

自旋锁（Spinlock）是一种用于实现多线程同步的简单锁机制。当一个线程试图获取自旋锁时，如果锁已经被其他线程持有，那么当前线程会一直执行循环（“自旋”）并不断检查锁的状态，直到锁变为可用状态。自旋锁不会让线程进入阻塞状态，因此不需要操作系统调度器介入。这使得自旋锁在某些场景下比其他锁机制更高效，特别是在锁持有时间较短的情况下。

自旋锁的优点：

1.  在锁持有时间较短的情况下，自旋锁通常比其他锁机制更高效，因为线程不会进入阻塞状态。这意味着不需要在阻塞/唤醒线程时花费额外的开销。
2.  自旋锁实现简单，通常只需要原子操作即可实现。

自旋锁的缺点：
1.  高竞争下的性能：当锁被持有时间较长或者多个线程竞争锁时，自旋锁的效率可能会降低。因为自旋锁会让线程一直消耗CPU时间来检查锁的状态，这可能导致CPU资源浪费。
2.  饥饿问题：在高竞争场景下，自旋锁可能导致某些线程长时间无法获取锁，从而导致饥饿问题。
3.  优先级反转问题：当高优先级线程等待低优先级线程释放锁时，高优先级线程可能长时间无法继续执行，导致优先级反转问题。

在实际应用中，自旋锁适用于以下场景：

1.  锁持有时间非常短。
2.  竞争锁的线程数量有限。
3.  系统中的线程优先级相近。

C++11中的`std::atomic_flag`可以用于实现自旋锁。以下是一个简单的自旋锁实现：
```cpp
#include <atomic>

class Spinlock {
public:
    void lock() {
        while (flag_.test_and_set(std::memory_order_acquire)) {
            // 自旋，等待锁释放
        }
    }

    void unlock() {
        flag_.clear(std::memory_order_release);
    }

private:
    std::atomic_flag flag_ = ATOMIC_FLAG_INIT;
};
```


### `std::atomic<bool>`

```cpp
std::atomic<bool> x(true);
x = false;
bool y = x.load(std::memory_order_acquire);  // 读取 x 值返回给 y
x.store(true);                               // x 写为 true
y = x.exchange(false,
               std::memory_order_acq_rel);  // x 用 false 替换，并返回旧值给 y
bool expected = false;                      // 期望值
// 不等则将期望值设为 x 并返回 false，相等则将 x 设为目标值 true 并返回 true
// weak 版本在相等时也可能替换失败而返回 false，因此一般用于循环
while (!x.compare_exchange_weak(expected, true) && !expected) {
}
// 对于只有两种值的 std::atomic<bool> 来说显得有些繁琐
// 但对其他原子类型来说，这个影响就大了
```


## 用整型原子类型实现Barrier

```cpp
#include <atomic>
#include <thread>

class Barrier {
 public:
  explicit Barrier(std::size_t n) : count_(n), spaces_(n), generation_(0) {}

  void wait() {
    std::size_t gen = generation_.load();
    if (--spaces_ == 0) {
      spaces_ = count_.load();
      ++generation_;
      return;
    }
    while (generation_.load() == gen) {
      std::this_thread::yield();
    }
  }

  void arrive() {
    --count_;
    if (--spaces_ == 0) {
      spaces_ = count_.load();
      ++generation_;
    }
  }

 private:
  std::atomic<std::size_t> count_;       // 需要同步的线程数
  std::atomic<std::size_t> spaces_;      // 剩余未到达 Barrier 的线程数
  std::atomic<std::size_t> generation_;  // 所有线程到达 Barrier 的总次数
};
```

***如果原子类型是自定义类型，那么该自定义类型必须可平凡复制（trivially copyable）***

c++20允许std::atomic的模板参数为std::shared_ptr
`std::atomic<std::shared_ptr<int>> x;`

### std::atomic::fence

`std::atomic::fence`是一个静态成员函数，用于在不涉及特定原子对象的情况下，在多线程程序中创建内存屏障（memory barrier）或内存栅栏（memory fence）。内存屏障用于限制编译器和处理器对内存操作进行重排序，以保证多线程程序的正确性。

在某些情况下，可能需要在不同的非原子操作之间创建内存屏障，以确保它们在多个线程中的执行顺序满足特定要求。`std::atomic::fence`提供了一种实现这种顺序保证的方法。它接受一个`std::memory_order`参数，用于指定内存屏障的强度。

以下是`std::atomic::fence`的一些用途：

1.  限制编译器和处理器对内存操作进行重排序。这对于确保多线程程序的正确性至关重要。
    
2.  强制刷新CPU缓存。在多核处理器系统中，每个核心可能有自己的缓存。内存屏障确保了在某个核心上的操作对其他核心可见，从而使整个系统的内存保持一致。
    
3.  在使用非原子操作进行同步时，保证它们的执行顺序满足特定要求。
    

以下是一个`std::atomic::fence`的示例：
```cpp
#include <atomic>
#include <iostream>
#include <thread>

int x = 0;
int y = 0;
std::atomic<int> sync(0);

void write() {
  x = 1;
  std::atomic_thread_fence(std::memory_order_release);
  sync.store(1, std::memory_order_relaxed);
}

void read() {
  while (sync.load(std::memory_order_relaxed) != 1) {
  }
  std::atomic_thread_fence(std::memory_order_acquire);
  std::cout << "y: " << y << ", x: " << x << std::endl;
}

int main() {
  std::thread t1(write);
  std::thread t2(read);
  t1.join();
  t2.join();
  return 0;
}
```
在此示例中，`write()`函数首先将`x`设为1，然后使用`std::memory_order_release`内存屏障确保`x`的写入在`sync`的写入之前完成。`read()`函数等待`sync`变为1，然后使用`std::memory_order_acquire`内存屏障确保`x`的读取在`sync`的读取之后进行。这样，我们保证了`write()`和`read()`函数之间的正确同步。

## 可平凡复制

在C++中，可平凡复制（Trivially Copyable）是一种类型特性，它描述了一种类型的复制操作可以通过简单地复制其内存表示（例如使用`memcpy`函数）来完成。这种类型的对象没有复杂的复制语义，例如用户定义的拷贝构造函数、拷贝赋值操作符或析构函数。

一个类型为可平凡复制的条件是：

1.  该类型具有平凡的拷贝构造函数（Trivial Copy Constructor）：意味着该类型没有用户定义的拷贝构造函数，也没有从其基类或非静态成员中继承非平凡拷贝构造函数。
2.  该类型具有平凡的拷贝赋值操作符（Trivial Copy Assignment Operator）：意味着该类型没有用户定义的拷贝赋值操作符，也没有从其基类或非静态成员中继承非平凡拷贝赋值操作符。
3.  该类型具有平凡的析构函数（Trivial Destructor）：意味着该类型没有用户定义的析构函数，也没有从其基类或非静态成员中继承非平凡析构函数。

对于可平凡复制的类型，编译器可以选择更高效的复制方式，例如使用内存复制函数`memcpy`。这种类型通常不包含任何资源管理、自定义分配器或复杂的状态管理。一些常见的可平凡复制类型的例子包括基本数据类型（如`int`、`float`等）和简单的结构体。

要检查一个类型是否为可平凡复制类型，可以使用C++11中的`std::is_trivially_copyable`类型特性：

```cpp
#include <type_traits>

struct SimpleStruct {
    int a;
    float b;
};

static_assert(std::is_trivially_copyable<SimpleStruct>::value, "SimpleStruct should be trivially copyable");
```

在这个例子中，`SimpleStruct`是一个可平凡复制的类型，因为它没有用户定义的拷贝构造函数、拷贝赋值操作符或析构函数。

在实际应用中，了解一个类型是否为可平凡复制有助于选择合适的复制策略。例如，在使用`std::copy`算法时，如果知道类型是可平凡复制的，可以放心地使用`memcpy`进行高效复制。

## 自定义原子类型与自由函数

-   自定义类型的原子类型不允许运算操作，只允许 [is_lock_free](https://en.cppreference.com/w/cpp/atomic/atomic/is_lock_free)、[load](https://en.cppreference.com/w/cpp/atomic/atomic/load)、[store](https://en.cppreference.com/w/cpp/atomic/atomic/store)、[exchange](https://en.cppreference.com/w/cpp/atomic/atomic/exchange)、[compare_exchange_weak、compare_exchange_strong](https://en.cppreference.com/w/cpp/atomic/atomic/compare_exchange)，以及赋值操作和向自定义类型转换的操作
-   除了每个类型各自的成员函数，[原子操作库](https://en.cppreference.com/w/cpp/atomic)还提供了通用的自由函数，只不过函数名多了一个 `atomic_` 前缀，参数变为指针类型
```cpp
std::atomic<int> i(42);
int j = std::atomic_load(&i);  // 等价于 i.load()
```

-   除 [std::atomic_is_lock_free](https://en.cppreference.com/w/cpp/atomic/atomic_is_lock_free) 外，每个自由函数有一个 `_explicit` 后缀版本，`_explicit` 自由函数额外接受一个内存序参数
```cpp
std::atomic<int> i(42);
std::atomic_load_explicit(&i, std::memory_order_acquire);  // i.load(std::memory_order_acquire)
```

-   自由函数的设计主要考虑的是 C 语言没有引用而只能使用指针，[compare_exchange_weak、compare_exchange_strong](https://en.cppreference.com/w/cpp/atomic/atomic/compare_exchange) 的第一个参数是引用，因此 [std::atomic_compare_exchange_weak、std::atomic_compare_exchange_strong](https://en.cppreference.com/w/cpp/atomic/atomic_compare_exchange) 的参数用的是指针

```cpp
bool compare_exchange_weak(T& expected, T desired, std::memory_order success,std::memory_order failure);

template <class T>
bool atomic_compare_exchange_weak(std::atomic<T>* obj,
typename std::atomic<T>::value_type* expected,
typename std::atomic<T>::value_type desired);

template <class T>
bool atomic_compare_exchange_weak_explicit(
    std::atomic<T>* obj, typename std::atomic<T>::value_type* expected,
    typename std::atomic<T>::value_type desired, std::memory_order succ,
    std::memory_order fail);
```


## 同步操作与强制排序

```cpp
std::vector<int> data;
std::atomic<bool> data_ready(false);

void read_thread() {
  while (!data_ready.load()) {  // 1 happens-before 2
    std::this_thread::sleep_for(std::chrono::milliseconds(1));
  }
  std::cout << data[0];  // 2
}

void write_thread() {
  data.emplace_back(42);  // 3 happens-before 4
  data_ready = true;      // 4 inter-thread happens-before 1
}
```

### synchronizes-with

-   synchronizes-with 关系只存在于原子类型操作上，如果一个数据结构包含原子类型，这个数据结构上的操作（比如加锁）也可能提供 synchronizes-with 关系
-   变量 x 上，标记了内存序的原子写操作 W，和标记了内存序的原子读操作，如果两者存在 synchronizes-with 关系，表示读操作读取的是：W 写入的值，或 W 之后同一线程上原子写操作写入 x 的值，或任意线程上对 x 的一系列原子读改写操作（比如 fetch_add、compare_exchange_weak）的值
-   简单来说，如果线程 A 写入一个值，线程 B 读取该值，则 A synchronizes-with B

### happens-before

指定某个操作可以看到另一个操作的结果

### inter-thread happens-before

-   如果一个线程中的操作 A [happens-before](https://en.cppreference.com/w/cpp/atomic/memory_order#Happens-before) 另一个线程中的操作 B，则 A [inter-thread happens-before](https://en.cppreference.com/w/cpp/atomic/memory_order#Inter-thread_happens-before) B
-   A [inter-thread happens-before](https://en.cppreference.com/w/cpp/atomic/memory_order#Inter-thread_happens-before) B 包括以下情况
    -   A synchronizes-with B
    -   A [dependency-ordered-before](https://en.cppreference.com/w/cpp/atomic/memory_order#Dependency-ordered_before) B
    -   A [inter-thread happens-before](https://en.cppreference.com/w/cpp/atomic/memory_order#Inter-thread_happens-before) X，X [inter-thread happens-before](https://en.cppreference.com/w/cpp/atomic/memory_order#Inter-thread_happens-before) B
    -   A [sequenced-before](https://en.cppreference.com/w/cpp/language/eval_order) X，X [inter-thread happens-before](https://en.cppreference.com/w/cpp/atomic/memory_order#Inter-thread_happens-before) B
    -   A synchronizes-with X，X [sequenced-before](https://en.cppreference.com/w/cpp/language/eval_order) B

### strongly happens-before


## 优先考虑基于任务的编程而不是基于线程的编程

thread-based
```cpp
int doAsyncWork();
std::thread t(doAsyncWork);
```
task-based
```cpp
auto fut = std::async(doAsyncWork); //“fut”表示“future”
```

基于任务的编程首先是代码量少，其次如果调用doAsyncWork的代码需要用到其返回值，基于线程的编程就无能为力了，而future还可以用到get函数

***基于任务的编程抽象层次更高***

有三种级别的线程，硬件线程软件线程和c++执行过程对象中的std::thread,它是软件线程的句柄（handle）

一旦资源超额，就会承受高昂的上下文切换开销，而一旦创建了大于系统支持的软件线程数量，就会抛出std::system_error,而使用std::async,你就将棘手的任务交给了c++标准库的开发者

调用std::async,不一定会开启一个新的线程，可能doAsyncWork被调度到了等待其结果的线程上

最前沿的线程调度器使用系统级线程池（_thread pool_）来避免资源超额的问题，并且通过工作窃取算法（_work-stealing algorithm_）来提升了跨硬件核心的负载均衡

## 使用std::thread更有优势的场景

### 需要访问非常基础的系统API

std::thread 提供了native_handle的成员函数

### 需要手动优化应用的线程使用

### 需要实现c++并发API之外的线程技术

比如c++实现中尚未支持的平台的线程池

总结：
-   `std::thread` API不能直接访问异步执行的结果，如果执行函数有异常抛出，代码会终止执行。
-   基于线程的编程方式需要手动的线程耗尽、资源超额、负责均衡、平台适配性管理。
-   通过带有默认启动策略的`std::async`进行基于任务的编程方式会解决大部分问题。


## 若要异步，则指定std::launch::threads

假定一个函数f传给std::async来执行
-   **`std::launch::async`启动策略**意味着`f`必须异步执行，即在不同的线程。
-   **`std::launch::deferred`启动策略**意味着`f`仅当在`std::async`返回的_future_上调用`get`或者`wait`时才执行。这表示`f`**推迟**到存在这样的调用时才执行（异步与并发是两个不同概念，这里侧重于惰性求值）。当`get`或`wait`被调用，`f`会同步执行，即调用方被阻塞，直到`f`运行结束。如果`get`和`wait`都没有被调用，`f`将不会被执行。

下面两种调用含义相同
```cpp
auto fut1 = std::async(f);                      //使用默认启动策略运行f
auto fut2 = std::async(std::launch::async |     //使用async或者deferred运行f
                       std::launch::deferred,
                       f);
```

但是使用默认启动策略的std::async也有一些有趣的影响

```cpp
auto fut = std::async(f);   //使用默认启动策略运行f
```

-   **无法预测`f`是否会与`t`并发运行**，因为`f`可能被安排延迟运行。
-   **无法预测`f`是否会在与某线程相异的另一线程上执行，这个某线程在`fut`上调用`get`或`wait`**。如果对`fut`调用函数的线程是`t`，含义就是无法预测`f`是否在异于`t`的另一线程上执行。
-   **无法预测`f`是否执行**，因为不能确保在程序每条路径上，都会不会在`fut`上调用`get`或者`wait`。

默认启动策略的调度灵活性导致使用`thread_local`变量比较麻烦，因为这意味着如果`f`读写了**线程本地存储**（_thread-local storage_，TLS），不可能预测到哪个线程的变量被访问
```cpp
auto fut = std::async(f);   //f的TLS可能是为单独的线程建的，
                            //也可能是为在fut上调用get或者wait的线程建的
```

下面是一个常见的错误

```cpp
using namespace std::literals;      //为了使用C++14中的时间段后缀；参见条款34

void f()                            //f休眠1秒，然后返回
{
    std::this_thread::sleep_for(1s);
}

auto fut = std::async(f);           //异步运行f（理论上）

while (fut.wait_for(100ms) !=       //循环，直到f完成运行时停止...
       std::future_status::ready)   //但是有可能永远不会发生！
{
    …
}
```

如果`f`是延迟执行，`fut.wait_for`将总是返回`std::future_status::deferred`。这永远不等于`std::future_status::ready`，循环会永远执行下去。因为机器资源超额或者线程耗尽时才容易出现延迟执行的现象，很容易火上浇油

所以我们需要检查f是否延迟执行了
```cpp
auto fut = std::async(f);               //同上

if (fut.wait_for(0s) ==                 //如果task是deferred（被延迟）状态
    std::future_status::deferred)
{
    …                                   //在fut上调用wait或get来异步调用f
} else {                                //task没有deferred（被延迟）
    while (fut.wait_for(100ms) !=       //不可能无限循环（假设f完成）
           std::future_status::ready) {
        …                               //task没deferred（被延迟），也没ready（已准备）
                                        //做并行工作直到已准备
    }
    …                                   //fut是ready（已准备）状态
}
```

所以最好在满足下面条件时才使用std::async的默认启动策略

-   任务不需要和执行`get`或`wait`的线程并行执行。
-   读写哪个线程的`thread_local`变量没什么问题。
-   可以保证会在`std::async`返回的_future_上调用`get`或`wait`，或者该任务可能永远不会执行也可以接受。
-   使用`wait_for`或`wait_until`编码时考虑到了延迟状态。

如果一个都满足不了，那么显式异步执行
```cpp
auto fut = std::async(std::launch::async, f);   //异步启动f的执行
```

```cpp
template<typename F, typename... Ts>
inline
auto                                        // C++14
reallyAsync(F&& f, Ts&&... params)
{
    return std::async(std::launch::async,
                      std::forward<F>(f),
                      std::forward<Ts>(params)...);
}
```


总结：
-   `std::async`的默认启动策略是异步和同步执行兼有的。
-   这个灵活性导致访问`thread_local`s的不确定性，隐含了任务可能不会被执行的意思，会影响调用基于超时的`wait`的程序逻辑。
-   如果异步执行任务非常关键，则指定`std::launch::async`。


## 让std::thread在所有路径最后都不可结合

每个`std::thread`对象处于两个状态之一：**可结合的**（_joinable_）或者**不可结合的**（_unjoinable_）。可结合状态的`std::thread`对应于正在运行或者可能要运行的异步执行线程。不可结合的`std::thread`对象包括：
-   **默认构造的`std::thread`s**。这种`std::thread`没有函数执行，因此没有对应到底层执行线程上。
-   **已经被移动走的`std::thread`对象**。移动的结果就是一个`std::thread`原来对应的执行线程现在对应于另一个`std::thread`。
-   **已经被`join`的`std::thread`** 。在`join`之后，`std::thread`不再对应于已经运行完了的执行线程。
-   **已经被`detach`的`std::thread`** 。`detach`断开了`std::thread`对象与执行线程之间的连接。

## 一些范例

### 高并发的thread-safe queue

```cpp
#include <condition_variable>
#include <memory>
#include <mutex>
#include <utility>

template <typename T>
class ConcurrentQueue {
 public:
  ConcurrentQueue() : head_(new Node), tail_(head_.get()) {}

  ConcurrentQueue(const ConcurrentQueue&) = delete;

  ConcurrentQueue& operator=(const ConcurrentQueue&) = delete;

  void push(T x) {
    auto new_val = std::make_shared<T>(std::move(x));
    auto new_node = std::make_unique<Node>();
    Node* new_tail_node = new_node.get();
    {
      std::lock_guard<std::mutex> l(tail_mutex_);
      tail_->v = new_val;
      tail_->next = std::move(new_node);
      tail_ = new_tail_node;
    }
    cv_.notify_one();
  }

  std::shared_ptr<T> try_pop() {
    std::unique_ptr<Node> head_node = try_pop_head();
    return head_node ? head_node->v : nullptr;
  }

  bool try_pop(T& res) {
    std::unique_ptr<Node> head_node = try_pop_head(res);
    return head_node != nullptr;
  }

  std::shared_ptr<T> wait_and_pop() {
    std::unique_ptr<Node> head_node = wait_pop_head();
    return head_node->v;
  }

  void wait_and_pop(T& res) { wait_pop_head(res); }

  bool empty() const {
    std::lock_guard<std::mutex> l(head_mutex_);
    return head_.get() == get_tail();
  }

 private:
  struct Node {
    std::shared_ptr<T> v;
    std::unique_ptr<Node> next;
  };

 private:
  std::unique_ptr<Node> try_pop_head() {
    std::lock_guard<std::mutex> l(head_mutex_);
    if (head_.get() == get_tail()) {
      return nullptr;
    }
    return pop_head();
  }

  std::unique_ptr<Node> try_pop_head(T& res) {
    std::lock_guard<std::mutex> l(head_mutex_);
    if (head_.get() == get_tail()) {
      return nullptr;
    }
    res = std::move(*head_->v);
    return pop_head();
  }

  std::unique_ptr<Node> wait_pop_head() {
    std::unique_lock<std::mutex> l(wait_for_data());
    return pop_head();
  }

  std::unique_ptr<Node> wait_pop_head(T& res) {
    std::unique_lock<std::mutex> l(wait_for_data());
    res = std::move(*head_->v);
    return pop_head();
  }

  std::unique_lock<std::mutex> wait_for_data() {
    std::unique_lock<std::mutex> l(head_mutex_);
    cv_.wait(l, [this] { return head_.get() != get_tail(); });
    return l;
  }

  std::unique_ptr<Node> pop_head() {
    std::unique_ptr<Node> head_node = std::move(head_);
    head_ = std::move(head_node->next);
    return head_node;
  }

  Node* get_tail() {
    std::lock_guard<std::mutex> l(tail_mutex_);
    return tail_;
  }

 private:
  std::unique_ptr<Node> head_;
  Node* tail_ = nullptr;
  std::mutex head_mutex_;
  mutable std::mutex tail_mutex_;
  std::condition_variable cv_;
};
```


这段代码是一个高并发线程安全的并发队列实现。其关键在于如何在保证线程安全的同时，尽可能减少锁的竞争，从而提高性能。以下几点使得这段代码的并发性能较高：

1.  分离的头尾锁：队列的头部和尾部分别使用独立的互斥锁保护。`head_mutex_`保护头部的访问，`tail_mutex_`保护尾部的访问。这种设计允许多个线程同时访问队列的头部和尾部，避免了全局锁带来的性能瓶颈。
    
2.  尽量减少锁的持有时间：在各个方法中，仅在必要的时候持有锁。例如，在`push`方法中，仅在修改尾节点时持有`tail_mutex_`。这样可以减少锁的竞争，提高并发性能。
    
3.  使用条件变量：`std::condition_variable`用于在`wait_and_pop`和`wait_pop_head`方法中等待数据。这可以确保在等待新数据时线程被阻塞，从而降低资源消耗。当新数据到达时，条件变量会唤醒一个等待的线程。
    
4.  使用智能指针管理资源：`std::unique_ptr`和`std::shared_ptr`用于管理队列节点及其数据。这可以确保资源的正确管理，避免内存泄漏，同时简化代码逻辑。
    

这段代码的设计在保证线程安全的同时，通过减少锁的竞争和合理的资源管理，实现了高并发性能。当然，对于特定的应用场景和需求，这个实现可能还有优化的空间。在实际应用中，可以根据具体需求进行调整。

### thread-safe map

```cpp
#include <algorithm>
#include <functional>
#include <list>
#include <map>
#include <memory>
#include <mutex>
#include <shared_mutex>
#include <utility>
#include <vector>

template <typename K, typename V, typename Hash = std::hash<K>>
class ConcurrentMap {
 public:
  // 桶数默认为 19（一般用 x % 桶数作为 x 的桶索引，桶数为质数可使桶分布均匀）
  ConcurrentMap(std::size_t n = 19, const Hash& h = Hash{})
      : buckets_(n), hasher_(h) {
    for (auto& x : buckets_) {
      x.reset(new Bucket);
    }
  }

  ConcurrentMap(const ConcurrentMap&) = delete;

  ConcurrentMap& operator=(const ConcurrentMap&) = delete;

  V get(const K& k, const V& default_value = V{}) const {
    return get_bucket(k).get(k, default_value);
  }

  void set(const K& k, const V& v) { get_bucket(k).set(k, v); }

  void erase(const K& k) { get_bucket(k).erase(k); }

  // 为了方便使用，提供一个到 std::map 的映射
  std::map<K, V> to_map() const {
    std::vector<std::unique_lock<std::shared_mutex>> locks;
    for (auto& x : buckets_) {
      locks.emplace_back(std::unique_lock<std::shared_mutex>(x->m));
    }
    std::map<K, V> res;
    for (auto& x : buckets_) {
      for (auto& y : x->data) {
        res.emplace(y);
      }
    }
    return res;
  }

 private:
  struct Bucket {
    std::list<std::pair<K, V>> data;
    mutable std::shared_mutex m;  // 每个桶都用这个锁保护

    V get(const K& k, const V& default_value) const {
      // 没有修改任何值，异常安全
      std::shared_lock<std::shared_mutex> l(m);  // 只读锁，可共享
      auto it = std::find_if(data.begin(), data.end(),
                             [&](auto& x) { return x.first == k; });
      return it == data.end() ? default_value : it->second;
    }

    void set(const K& k, const V& v) {
      std::unique_lock<std::shared_mutex> l(m);  // 写，单独占用
      auto it = std::find_if(data.begin(), data.end(),
                             [&](auto& x) { return x.first == k; });
      if (it == data.end()) {
        data.emplace_back(k, v);  // emplace_back 异常安全
      } else {
        it->second = v;  // 赋值可能抛异常，但值是用户提供的，可放心让用户处理
      }
    }

    void erase(const K& k) {
      std::unique_lock<std::shared_mutex> l(m);  // 写，单独占用
      auto it = std::find_if(data.begin(), data.end(),
                             [&](auto& x) { return x.first == k; });
      if (it != data.end()) {
        data.erase(it);
      }
    }
  };

  Bucket& get_bucket(const K& k) const {  // 桶数固定因此可以无锁调用
    return *buckets_[hasher_(k) % buckets_.size()];
  }

 private:
  std::vector<std::unique_ptr<Bucket>> buckets_;
  Hash hasher_;
};
```


这段代码实现了一个线程安全的并发哈希映射（`ConcurrentMap`），它在保证线程安全的基础上实现了高并发。下面我们将详细阐述该程序的优缺点。

优点：

1.  使用桶：`ConcurrentMap`通过将哈希映射分成多个桶来实现高并发。每个桶都有自己的互斥锁（`std::shared_mutex`），这使得多个线程可以同时访问不同的桶，从而提高并发性能。
    
2.  使用共享互斥锁：每个桶使用`std::shared_mutex`来保护自己。`std::shared_mutex`允许多个线程同时获取共享锁（即读锁），但在任何时候只允许一个线程获取独占锁（即写锁）。这样，多个读线程可以同时访问同一个桶，从而提高并发性能。
    
3.  使用智能指针：`std::unique_ptr`用于管理桶的资源，避免内存泄漏，同时简化代码逻辑。
    
4.  通过使用STL容器和算法，代码易于理解和维护。
    

缺点：

1.  有限的并发性能：虽然`ConcurrentMap`提供了高并发性能，但它仍然受到桶数的限制。当许多线程需要访问相同的桶时，性能可能会下降。
    
2.  哈希函数和桶数对性能的影响：默认的桶数和哈希函数可能不适用于所有用例。为了获得最佳性能，可能需要根据实际数据分布和访问模式调整桶数和哈希函数。
    
3.  不支持动态扩容：当映射变得非常大时，`ConcurrentMap`不会自动调整桶数，这可能导致性能下降。要解决这个问题，可以实现动态扩容的功能，但这会增加实现的复杂性。
    
4.  内存占用：由于每个桶都有一个互斥锁，当桶数很大时，内存占用可能会成为问题。
    

总的来说，这段代码实现了一个基本的线程安全并发哈希映射，其优点是高并发性能、使用智能指针管理资源以及易于理解和维护。缺点主要是受桶数限制的并发性能、哈希函数和桶数对性能的影响、不支持动态扩容以及内存占用。在实际应用中，可以根据具体需求对这个实现进行优化和调整。

### thread-safe list

```cpp
#include <memory>
#include <mutex>
#include <utility>

template <typename T>
class ConcurrentList {
 public:
  ConcurrentList() = default;

  ~ConcurrentList() {
    remove_if([](const Node&) { return true; });
  }

  ConcurrentList(const ConcurrentList&) = delete;

  ConcurrentList& operator=(const ConcurrentList&) = delete;

  void push_front(const T& x) {
    std::unique_ptr<Node> t(new Node(x));
    std::lock_guard<std::mutex> head_lock(head_.m);
    t->next = std::move(head_.next);
    head_.next = std::move(t);
  }

  template <typename F>
  void for_each(F f) {
    Node* cur = &head_;
    std::unique_lock<std::mutex> head_lock(head_.m);
    while (Node* const next = cur->next.get()) {
      std::unique_lock<std::mutex> next_lock(next->m);
      head_lock.unlock();  // 锁住了下一节点，因此可以释放上一节点的锁
      f(*next->data);
      cur = next;                        // 当前节点指向下一节点
      head_lock = std::move(next_lock);  // 转交下一节点锁的所有权，循环上述过程
    }
  }

  template <typename F>
  std::shared_ptr<T> find_first_if(F f) {
    Node* cur = &head_;
    std::unique_lock<std::mutex> head_lock(head_.m);
    while (Node* const next = cur->next.get()) {
      std::unique_lock<std::mutex> next_lock(next->m);
      head_lock.unlock();
      if (f(*next->data)) {
        return next->data;  // 返回目标值，无需继续查找
      }
      cur = next;
      head_lock = std::move(next_lock);
    }
    return nullptr;
  }

  template <typename F>
  void remove_if(F f) {
    Node* cur = &head_;
    std::unique_lock<std::mutex> head_lock(head_.m);
    while (Node* const next = cur->next.get()) {
      std::unique_lock<std::mutex> next_lock(next->m);
      if (f(*next->data)) {  // 为 true 则移除下一节点
        std::unique_ptr<Node> old_next = std::move(cur->next);
        cur->next = std::move(next->next);  // 下一节点设为下下节点
        next_lock.unlock();
      } else {  // 否则继续转至下一节点
        head_lock.unlock();
        cur = next;
        head_lock = std::move(next_lock);
      }
    }
  }

 private:
  struct Node {
    std::mutex m;
    std::shared_ptr<T> data;
    std::unique_ptr<Node> next;
    Node() = default;
    Node(const T& x) : data(std::make_shared<T>(x)) {}
  };

  Node head_;
};
```

这段代码是一个线程安全的并发链表实现。它在保证线程安全的基础上，通过逐个锁定链表节点的方式实现了高并发。

优点：

1.  逐个锁定节点：该实现的关键在于锁定链表中的每个节点。这意味着可以在访问链表的多个部分时允许其他线程并发执行。例如，在`for_each`函数中，通过循环将每个节点的互斥锁逐个锁定，这样可以避免全局锁的性能瓶颈。
    
2.  锁的所有权转移：通过使用`std::unique_lock`，可以轻松地将锁的所有权从一个锁传递到另一个锁。这使得代码更加简洁，并降低了可能出错的机会。
    
3.  使用智能指针管理资源：代码中使用了`std::shared_ptr`和`std::unique_ptr`来管理链表节点及其数据。这样可以确保资源的正确管理，避免内存泄漏，同时简化了代码逻辑。
    

缺点：

1.  高并发场景下的性能：虽然这个实现通过逐个锁定节点来实现并发性能，但在高并发场景下，当多个线程同时访问链表时，可能会导致链表的前部分成为瓶颈。这是因为在某些情况下，多个线程可能需要等待同一个节点的锁。
    
2.  内存开销：由于每个节点都有一个互斥锁，这会增加链表的内存开销。在有大量节点的情况下，这可能成为一个问题。
    
3.  代码复杂度：与非线程安全的链表实现相比，这个实现具有更高的代码复杂度。虽然这是为了确保线程安全，但对于某些可能不需要高并发性能的应用程序来说，这可能是一个过度设计。
    

总之，这个实现在保证线程安全的同时实现了高并发性能。然而，在某些情况下，这种实现可能存在性能瓶颈，并可能导致更高的内存开销。在实际应用中，根据具体需求选择或设计合适的并发数据结构是很重要的。

## 非阻塞数据结构

-   阻塞的算法和数据结构使用 mutex、条件变量、期值来同步数据，但非阻塞不等价于 lock-free，比如自旋锁没有使用任何阻塞函数的调用，是非阻塞的，但并非 lock-free
-   非阻塞数据结构由松到严可分为三个等级：obstruction-free、lock-free、wait-free
    -   obstruction-free（无障碍）：如果其他线程都暂停了，任何一个给定的线程都会在有限步数内完成操作。上例就是这种情况，但这种情况很少见，所以满足这个条件只能算一个失败的 lock-free 实现
    -   lock-free（无锁）：如果多线程在同一个数据结构上操作，其中一个将在有限步数内完成操作。满足 lock-free 必定满足 obstruction-free
    -   wait-free（无等待）：如果多线程在同一个数据结构上操作，每个线程都会在有限步数内完成操作。满足 wait-free 必定满足 lock-free，但 wait-free 很难实现，因为要保证有限步数内完成操作，就要保证操作一次通过，并且执行到某一步不能导致其他线程操作失败
-   lock-free 数据结构必须允许多线程并发访问，但它们不能做相同操作，比如一个 lock-free 的 queue 允许一个线程 push、另一个线程 pop，但不允许两个线程同时 push。此外，如果一个访问 lock-free 数据结构的线程被中途挂起，其他线程必须能完成操作而不需要等待挂起的线程
-   使用 lock-free 数据结构主要是为了最大化并发访问，不需要阻塞。第二个原因是鲁棒性，如果线程在持有锁时死掉就会导致数据结构被永久破坏，而对 lock-free 数据结构来说，除了死掉的线程里的数据，其他的数据都不会丢失。lock-free 没有任何锁，所以一定不会出现死锁
-   但 lock-free 可能造成更大开销，用于 lock-free 的原子操作比非原子操作慢得多，且 lock-free 数据结构中的原子操作一般比 lock-based 中的多，此外，硬件必须访问同一个原子变量以在线程间同步数据。无论 lock-free 还是 lock-based，性能方面的检查（最坏情况等待时间、平均等待时间、总体执行时间或其他方面）都是非常重要的

### lock-free thread-safe stack

-   最简单的 stack 实现方式是包含头节点指针的链表。push 的过程很简单，创建一个新节点，然后让新节点的 next 指针指向当前 head，最后 head 设为新节点
-   这里的 race condition 在于，如果两个线程同时 push，让各自的新节点的 next 指针指向当前 head，这样必然导致 head 最终设为二者之一的新节点，而另一个被丢弃
-   解决方法是，在最后设置 head 时先进行判断，只有当前 head 与新节点的 next 相等，才将 head 设为新节点，如果不等则让 next 指向当前 head 并重新判断。而这个操作必须是原子的，因此就需要使用 [compare_exchange_weak](https://en.cppreference.com/w/cpp/atomic/atomic/compare_exchange)，不需要使用 [compare_exchange_strong](https://en.cppreference.com/w/cpp/atomic/atomic/compare_exchange)，因为 [compare_exchange_weak](https://en.cppreference.com/w/cpp/atomic/atomic/compare_exchange) 在相等时可能替换失败，但替换失败也会返回 false，放在循环里带来的效果是一样的，而 [compare_exchange_weak](https://en.cppreference.com/w/cpp/atomic/atomic/compare_exchange) 在一些机器架构上可以产生比 [compare_exchange_strong](https://en.cppreference.com/w/cpp/atomic/atomic/compare_exchange) 更优化的代码

```cpp
#include <atomic>

template <typename T>
class LockFreeStack {
 public:
  void push(const T& x) {
    Node* t = new Node(x);
    t->next = head_.load();
    while (!head_.compare_exchange_weak(t->next, t)) {
    }
  }

 private:
  struct Node {
    T v;
    Node* next = nullptr;
    Node(const T& x) : v(x) {}
  };

 private:
  std::atomic<Node*> head_;
};
```

-   pop 的过程很简单，先存储当前头节点指针，再将头节点设为下一节点，最后返回存储的头节点并删除指针。这里的 race condition 在于，如果两个线程同时 pop，如果一个已经删除了头节点，另一个线程读取头节点的下一节点就访问了空悬指针
-   先绕开删除指针这一步，考虑前几步的实现
```cpp
template <typename T>
void LockFreeStack<T>::pop(T& res) {
  Node* t = head_.load();  // 未考虑头节点为空指针的情况
  while (!head_.compare_exchange_weak(t, t->next)) {
  }
  res = t->v;
}
```

-   传引用来保存结果的原因是，如果直接返回值，返回前一定会先移除元素，如果拷贝返回值时抛出异常，移除的元素就丢失了。但传引用的问题是，如果其他线程移除了节点，被移除的节点不能被解引用，当前线程就无法安全地拷贝数据。因此，如果想安全地返回值，应该返回智能指针

```cpp
#include <atomic>
#include <memory>

template <typename T>
class LockFreeStack {
 public:
  void push(const T& x) {
    Node* t = new Node(x);
    t->next = head_.load();
    while (!head_.compare_exchange_weak(t->next, t)) {
    }
  }

  std::shared_ptr<T> pop() {  // 还未考虑释放原来的头节点指针
    Node* t = head_.load();
    while (t && !head_.compare_exchange_weak(t, t->next)) {
    }
    return t ? t->v : nullptr;
  }

 private:
  struct Node {
    std::shared_ptr<T> v;
    Node* next = nullptr;
    Node(const T& x) : v(std::make_shared<T>(x)) {}
  };

 private:
  std::atomic<Node*> head_;
};
```


-   释放被移除的节点的难点在于，一个线程在释放内存时，无法得知其他线程是否持有要释放的指针
-   只要没有其他线程调用 pop，就能安全释放，因此可以用一个计数器来记录调用 pop 的线程数，计数不为 1 时，先把节点添加到待删除节点列表中，计数为 1 则安全释放
```cpp
#include <atomic>
#include <memory>

template <typename T>
class LockFreeStack {
 public:
  void push(const T& x) {
    Node* t = new Node(x);
    t->next = head_.load();
    while (!head_.compare_exchange_weak(t->next, t)) {
    }
  }

  std::shared_ptr<T> pop() {
    ++pop_cnt_;
    Node* t = head_.load();
    while (t && !head_.compare_exchange_weak(t, t->next)) {
    }
    std::shared_ptr<T> res;
    if (t) {
      res.swap(t->v);
    }
    try_delete(t);
    return res;
  }

 private:
  struct Node {
    std::shared_ptr<T> v;
    Node* next = nullptr;
    Node(const T& x) : v(std::make_shared<T>(x)) {}
  };

 private:
  static void delete_list(Node* head) {
    while (head) {
      Node* t = head->next;
      delete head;
      head = t;
    }
  }

  void append_to_delete_list(Node* first, Node* last) {
    last->next = to_delete_list_;
    // 确保 last->next 为 to_delete_list_，再设置 first 为新的头节点
    while (!to_delete_list_.compare_exchange_weak(last->next, first)) {
    }
  }

  void append_to_delete_list(Node* head) {
    Node* last = head;
    while (Node* t = last->next) {
      last = t;
    }
    append_to_delete_list(head, last);
  }

  void try_delete(Node* head) {
    if (pop_cnt_ == 0) {
      return;
    }
    if (pop_cnt_ > 1) {
      append_to_delete_list(head, head);
      --pop_cnt_;
      return;
    }
    Node* t = to_delete_list_.exchange(nullptr);
    if (--pop_cnt_ == 0) {
      delete_list(t);
    } else if (t) {
      append_to_delete_list(t);
    }
    delete head;
  }

 private:
  std::atomic<Node*> head_;
  std::atomic<std::size_t> pop_cnt_;
  std::atomic<Node*> to_delete_list_;
};
```

-   如果要释放所有节点，必须有一个时刻计数器为 0。在高负载的情况下，往往不会存在这样的时刻，从而导致待删除节点的列表无限增长

### Hazard Pointer（风险指针）

-   另一个释放的思路是，在线程访问节点时，设置一个保存了线程 ID 和该节点的风险指针。用一个全局数组保存所有线程的风险指针，释放节点时，如果数组中不存在包含该节点的风险指针，则可以直接释放，否则将节点添加到待删除列表中。风险指针实现如下
```cpp
#include <atomic>
#include <stdexcept>
#include <thread>

static constexpr std::size_t MaxSize = 100;

struct HazardPointer {
  std::atomic<std::thread::id> id;
  std::atomic<void*> p;
};

static HazardPointer HazardPointers[MaxSize];

class HazardPointerHelper {
 public:
  HazardPointerHelper() {
    for (auto& x : HazardPointers) {
      std::thread::id default_id;
      if (x.id.compare_exchange_strong(default_id,
                                       std::this_thread::get_id())) {
        hazard_pointer = &x;  // 取一个未设置过的风险指针
        break;
      }
    }
    if (!hazard_pointer) {
      throw std::runtime_error("No hazard pointers available");
    }
  }

  ~HazardPointerHelper() {
    hazard_pointer->p.store(nullptr);
    hazard_pointer->id.store(std::thread::id{});
  }

  HazardPointerHelper(const HazardPointerHelper&) = delete;

  HazardPointerHelper operator=(const HazardPointerHelper&) = delete;

  std::atomic<void*>& get() { return hazard_pointer->p; }

 private:
  HazardPointer* hazard_pointer = nullptr;
};

std::atomic<void*>& hazard_pointer_for_this_thread() {
  static thread_local HazardPointerHelper t;
  return t.get();
}

bool is_existing(void* p) {
  for (auto& x : HazardPointers) {
    if (x.p.load() == p) {
      return true;
    }
  }
  return false;
}
```

使用风险指针

```cpp
#include <atomic>
#include <functional>
#include <memory>

#include "hazard_pointer.hpp"

template <typename T>
class LockFreeStack {
 public:
  void push(const T& x) {
    Node* t = new Node(x);
    t->next = head_.load();
    while (!head_.compare_exchange_weak(t->next, t)) {
    }
  }

  std::shared_ptr<T> pop() {
    std::atomic<void*>& hazard_pointer = hazard_pointer_for_this_thread();
    Node* t = head_.load();
    do {  // 外循环确保 t 为最新的头节点，循环结束后将头节点设为下一节点
      Node* t2;
      do {  // 循环至风险指针保存当前最新的头节点
        t2 = t;
        hazard_pointer.store(t);
        t = head_.load();
      } while (t != t2);
    } while (t && !head_.compare_exchange_strong(t, t->next));
    hazard_pointer.store(nullptr);
    std::shared_ptr<T> res;
    if (t) {
      res.swap(t->v);
      if (is_existing(t)) {
        append_to_delete_list(new DataToDelete{t});
      } else {
        delete t;
      }
      try_delete();
    }
    return res;
  }

 private:
  struct Node {
    std::shared_ptr<T> v;
    Node* next = nullptr;
    Node(const T& x) : v(std::make_shared<T>(x)) {}
  };

  struct DataToDelete {
    template <typename T>
    DataToDelete(T* p)
        : data(p), deleter([](void* p) { delete static_cast<T*>(p); }) {}

    ~DataToDelete() { deleter(data); }

    void* data = nullptr;
    std::function<void(void*)> deleter;
    DataToDelete* next = nullptr;
  };

 private:
  void append_to_delete_list(DataToDelete* t) {
    t->next = to_delete_list_.load();
    while (!to_delete_list_.compare_exchange_weak(t->next, t)) {
    }
  }

  void try_delete() {
    DataToDelete* cur = to_delete_list_.exchange(nullptr);
    while (cur) {
      DataToDelete* t = cur->next;
      if (!is_existing(cur->data)) {
        delete cur;
      } else {
        append_to_delete_list(new DataToDelete{cur});
      }
      cur = t;
    }
  }

 private:
  std::atomic<Node*> head_;
  std::atomic<std::size_t> pop_cnt_;
  std::atomic<DataToDelete*> to_delete_list_;
};
```


-   风险指针实现简单并达到了安全释放的目的，但每次删除节点前后都要遍历数组并原子访问内部指针来检查，增加了很多开销
-   无锁内存回收技术领域十分活跃，大公司都会申请自己的专利，风险指针包含在 IBM 提交的专利申请中，在 GPL 协议下允许免费使用

### 引用计数

-   另一个方案是使用引用计数记录访问每个节点的线程数量，[std::shared_ptr](https://en.cppreference.com/w/cpp/memory/shared_ptr) 的操作是原子的，但要检查是否 lock-free
```cpp
std::shared_ptr<int> p(new int(42));
assert(std::atomic_is_lock_free(&p));
```

-   如果是，则可以用于实现 lock-free stack(vs2022上面std::atomic<std::shared_ptr>并不lock-free)
```cpp
#include <atomic>
#include <memory>

template <typename T>
class LockFreeStack {
 public:
  ~LockFreeStack() {
    while (pop()) {
    }
  }

  void push(const T& x) {
    auto t = std::make_shared<Node>(x);
    t->next = head_.load();
    while (!head_.compare_exchange_weak(t->next, t)) {
    }
  }

  std::shared_ptr<T> pop() {
    std::shared_ptr<Node> t = head_.load();
    while (t && !head_.compare_exchange_weak(t, t->next.load())) {
    }
    if (t) {
      t->next = std::shared_ptr<Node>();
      return t->v;
    }
    return nullptr;
  }

 private:
  struct Node {
    std::shared_ptr<T> v;
    std::atomic<std::shared_ptr<Node>> next;
    Node(const T& x) : v(std::make_shared<T>(x)) {}
  };

 private:
  std::atomic<std::shared_ptr<Node>> head_;
};
```

-   更通用的方法是手动管理引用计数，为每个节点设置内外部两个引用计数，两者之和就是节点的引用计数，外部计数默认为 1，访问对象时递增外部计数并递减内部计数，访问结束后则不再需要外部计数，将外部计数减 2 并加到内部计数上
```cpp
#include <atomic>
#include <memory>

template <typename T>
class LockFreeStack {
 public:
  ~LockFreeStack() {
    while (pop()) {
    }
  }

  void push(const T& x) {
    ReferenceCount t;
    t.p = new Node(x);
    t.external_cnt = 1;
    t.p->next = head_.load();
    while (!head_.compare_exchange_weak(t.p->next, t)) {
    }
  }

  std::shared_ptr<T> pop() {
    ReferenceCount t = head_.load();
    while (true) {
      increase_count(t);  // 外部计数递增表示该节点正被使用
      Node* p = t.p;      // 因此可以安全地访问
      if (!p) {
        return nullptr;
      }
      if (head_.compare_exchange_strong(t, p->next)) {
        std::shared_ptr<T> res;
        res.swap(p->v);
        // 将外部计数减 2 后加到内部计数，减 2 是因为，
        // 节点被删除减 1，该线程无法再次访问此节点再减 1
        const int cnt = t.external_cnt - 2;
        if (p->inner_cnt.fetch_add(cnt) == -cnt) {
          delete p;  // 内外部计数和为 0
        }
        return res;
      }
      if (p->inner_cnt.fetch_sub(1) == 1) {
        delete p;  // 内部计数为 0
      }
    }
  }

 private:
  struct Node;

  struct ReferenceCount {
    int external_cnt;
    Node* p;
  };

  struct Node {
    std::shared_ptr<T> v;
    std::atomic<int> inner_cnt = 0;
    ReferenceCount next;
    Node(const T& x) : v(std::make_shared<T>(x)) {}
  };

  void increase_count(ReferenceCount& old_cnt) {
    ReferenceCount new_cnt;
    do {
      new_cnt = old_cnt;
      ++new_cnt.external_cnt;  // 访问 head_ 时递增外部计数，表示该节点正被使用
    } while (!head_.compare_exchange_strong(old_cnt, new_cnt));
    old_cnt.external_cnt = new_cnt.external_cnt;
  }

 private:
  std::atomic<ReferenceCount> head_;
};
```

-   不指定内存序则默认使用开销最大的 `std::memory_order_seq_cst`，下面根据操作间的依赖关系优化为最小内存序
```cpp
#include <atomic>
#include <memory>

template <typename T>
class LockFreeStack {
 public:
  ~LockFreeStack() {
    while (pop()) {
    }
  }

  void push(const T& x) {
    ReferenceCount t;
    t.p = new Node(x);
    t.external_cnt = 1;
    // 下面比较中 release 保证之前的语句都先执行，因此 load 可以使用 relaxed
    t.p->next = head_.load(std::memory_order_relaxed);
    while (!head_.compare_exchange_weak(t.p->next, t, std::memory_order_release,
                                        std::memory_order_relaxed)) {
    }
  }

  std::shared_ptr<T> pop() {
    ReferenceCount t = head_.load(std::memory_order_relaxed);
    while (true) {
      increase_count(t);  // acquire
      Node* p = t.p;
      if (!p) {
        return nullptr;
      }
      if (head_.compare_exchange_strong(t, p->next,
                                        std::memory_order_relaxed)) {
        std::shared_ptr<T> res;
        res.swap(p->v);
        // 将外部计数减 2 后加到内部计数，减 2 是因为，
        // 节点被删除减 1，该线程无法再次访问此节点再减 1
        const int cnt = t.external_cnt - 2;
        // swap 要先于 delete，因此使用 release
        if (p->inner_cnt.fetch_add(cnt, std::memory_order_release) == -cnt) {
          delete p;  // 内外部计数和为 0
        }
        return res;
      }
      if (p->inner_cnt.fetch_sub(1, std::memory_order_relaxed) == 1) {
        p->inner_cnt.load(std::memory_order_acquire);  // 只是用 acquire 来同步
        // acquire 保证 delete 在之后执行
        delete p;  // 内部计数为 0
      }
    }
  }

 private:
  struct Node;

  struct ReferenceCount {
    int external_cnt;
    Node* p = nullptr;
  };

  struct Node {
    std::shared_ptr<T> v;
    std::atomic<int> inner_cnt = 0;
    ReferenceCount next;
    Node(const T& x) : v(std::make_shared<T>(x)) {}
  };

  void increase_count(ReferenceCount& old_cnt) {
    ReferenceCount new_cnt;
    do {  // 比较失败不改变当前值，并可以继续循环，因此可以选择 relaxed
      new_cnt = old_cnt;
      ++new_cnt.external_cnt;  // 访问 head_ 时递增外部计数，表示该节点正被使用
    } while (!head_.compare_exchange_strong(old_cnt, new_cnt,
                                            std::memory_order_acquire,
                                            std::memory_order_relaxed));
    old_cnt.external_cnt = new_cnt.external_cnt;
  }

 private:
  std::atomic<ReferenceCount> head_;
};
```

## 如何划分工作数据

### 递归式
![[Pasted image 20230408180015.png]]

```cpp
#include <algorithm>
#include <atomic>
#include <future>
#include <list>
#include <memory>
#include <thread>
#include <vector>

#include "concurrent_stack.hpp"

template <typename T>
class Sorter {
 public:
  Sorter() : max_thread_count(std::thread::hardware_concurrency() - 1) {}

  ~Sorter() {
    end_of_data = true;
    for (auto& x : threads) {
      if (x.joinable()) {
        x.join();
      }
    }
  }

  std::list<T> do_sort(std::list<T>& v) {
    if (v.empty()) {
      return {};
    }
    std::list<T> res;
    res.splice(res.begin(), v, v.begin());
    auto it = std::partition(v.begin(), v.end(),
                             [&](const T& x) { return x < res.front(); });
    ChunkToSort low;
    low.data.splice(low.data.end(), v, v.begin(), it);
    std::future<std::list<T>> l = low.promise.get_future();
    chunks.push(std::move(low));
    if (threads.size() < max_thread_count) {
      threads.emplace_back(&Sorter<T>::sort_thread, this);
    }
    auto r{do_sort(v)};
    res.splice(res.end(), r);
    while (l.wait_for(std::chrono::seconds(0)) != std::future_status::ready) {
      try_sort_chunk();
    }
    res.splice(res.begin(), l.get());
    return res;
  }

 private:
  struct ChunkToSort {
    std::list<T> data;
    std::promise<std::list<T>> promise;
  };

 private:
  void sort_chunk(const std::shared_ptr<ChunkToSort>& chunk) {
    chunk->promise.set_value(do_sort(chunk->data));
  }

  void try_sort_chunk() {
    std::shared_ptr<ChunkToSort> chunk = chunks.pop();
    if (chunk) {
      sort_chunk(chunk);
    }
  }

  void sort_thread() {
    while (!end_of_data) {
      try_sort_chunk();
      std::this_thread::yield();
    }
  }

 private:
  ConcurrentStack<ChunkToSort> chunks;
  std::vector<std::thread> threads;
  const std::size_t max_thread_count;
  std::atomic<bool> end_of_data = false;
};

template <typename T>
std::list<T> parallel_quick_sort(std::list<T> v) {
  if (v.empty()) {
    return {};
  }
  return Sorter<T>{}.do_sort(v);
}
```

这段代码实现了一个并行快速排序算法。在这个实现中，使用了 C++11 新增的一些特性，如原子操作、并发库和智能指针。下面将详细分析代码所做的事情以及其优缺点：

这个算法的主要实现是通过 Sorter 类完成的，它主要包含以下几个部分：

1.  `do_sort` 函数：这个函数是递归的，它将输入的列表分成两部分，分别是小于基准值的元素和大于等于基准值的元素。对于每一个小于基准值的元素，创建一个新的 `ChunkToSort` 对象，然后将该对象放入 `chunks` 并发栈中。每个 `ChunkToSort` 对象包含一个未排序的数据列表和一个用于存储排序结果的 `std::promise` 对象。
    
2.  `try_sort_chunk` 函数：这个函数尝试从 `chunks` 栈中弹出一个 `ChunkToSort` 对象。如果栈中有数据，该函数将对弹出的数据进行排序，并将排序结果通过 `std::promise` 对象设置给对应的 `std::future` 对象。
    
3.  `sort_thread` 函数：这是一个线程函数，它在一个循环中不断地调用 `try_sort_chunk` 函数，直到 `end_of_data` 标志变为 `true`。
    
4.  `parallel_quick_sort` 函数：这是算法的入口函数，它创建一个 `Sorter` 对象并调用其 `do_sort` 函数完成排序。
    

优点：

1.  利用多核处理器进行并行排序，提高了排序性能。
2.  使用智能指针管理内存，避免了内存泄漏。
3.  使用并发库中的 `std::promise` 和 `std::future` 实现异步操作，使得代码更加简洁和易于理解。
4.  使用原子操作和线程库实现线程同步和控制，减少了潜在的竞争条件。

缺点：

1.  代码复杂度相对较高，对于不熟悉 C++11 新特性的开发者来说，理解起来可能有些困难。
2.  需要合理设置线程数量，否则可能导致性能下降。在这个实现中，线程数量是通过 `std::thread::hardware_concurrency()` 函数获取的，但实际情况下可能需要根据硬件和任务特点来调整。
3.  递归实现可能导致栈溢出，特别是在处理大量数据时。在某些情况下，可以考虑使用迭代方式实现快速排序算法。

总的来说，这段代码是一个并行快速排序算法的实现，它充分利用了 C++11 的新特性，提高了排序性能。但是，代码的复杂度相对较高，需要对 C++11 的新特性有一定的了解才能更好地理解代码。此外，代码在处理大量数据时可能会遇到栈溢出的问题，因为递归实现可能导致栈空间的快速消耗。为了解决这个问题，可以考虑使用迭代方式实现快速排序算法。

在实际应用中，除了硬件并发级别，也需要考虑任务的特点和具体数据分布来确定合适的线程数量。过多的线程可能导致线程间的竞争和上下文切换，从而降低性能。为了获得更好的性能，可以根据具体情况调整线程数量，或者使用线程池等技术来提高线程的复用。

总之，这段代码实现了一个并行快速排序算法，通过充分利用多核处理器和 C++11 的新特性来提高排序性能。在使用时，需要注意线程数量的设置以及递归实现可能带来的栈溢出问题。同时，在实际应用中，可以根据数据量和任务特点进行优化，以获得更好的性能。


### 基于任务划分

如果数据是来自于外部或者动态生成的，基于递归的划分就失去意义了，这种情况下适合让不同的线程做不同的工作，用户接口线程处理用户结构，后台任务线程关注自己的任务。基于任务的划分不需要完全的隔离，如果要处理的多个数据集都适用一套相同顺序的操作，我们就可以画出一道流水线
```cpp
// 非 pipeline：每 20 秒 4 个数据（每个数据仍要 20 秒）
线程A：-1- -1- -1- -1- -5- -5- -5- -5-
线程B：-2- -2- -2- -2- -6- -6- -6- -6-
线程C：-3- -3- -3- -3- -7- -7- -7- -7-
线程D：-4- -4- -4- -4- -8- -8- -8- -8-

// pipeline：第一个数据 20 秒，之后每个 5 秒
线程A：-1- -2- -3- -4- -5- -6- -7- -8-
线程B：--- -1- -2- -3- -4- -5- -6- -7-
线程C：--- --- -1- -2- -3- -4- -5- -6-
线程D：--- --- --- -1- -2- -3- -4- -5-
```

## 影响并发代码性能的因素

### 处理器数量

可以用std::hardware_concurrency来获取硬件支持的线程数，然而它不会考虑已经运行在机器上的线程数，过度依赖它可能导致oversubscription,解决这个问题可以考虑std::async,也可以用线程池

### 乒乓缓存

乒乓缓存现象（Cache Ping-Pong）是一种多核处理器中多个线程或核心在访问共享数据时发生的缓存竞争现象。当多个线程或核心频繁地读取和写入相同的内存位置时，缓存行（Cache Line）会在各个处理器核心的缓存之间来回传递，导致性能下降。这种现象得名于乒乓球比赛中球在两名选手之间不断地来回传递的情景。

乒乓缓存现象的发生有以下几个原因：

1.  共享数据：当多个线程或核心同时访问相同的内存位置时，它们的缓存之间需要保持一致性。这会导致缓存行在处理器核心之间传递，增加通信开销，从而降低性能。
    
2.  缓存行大小：现代处理器的缓存行通常为 64 字节或 128 字节。如果多个线程在同一个缓存行内的不同变量上工作，即使它们访问的是独立的数据，也可能触发乒乓缓存现象。
    
3.  缓存一致性协议：多核处理器使用缓存一致性协议（如 MESI、MOESI 或 MESIF）来保证各个核心缓存中的数据一致性。这些协议在处理共享数据时，需要在缓存之间传递消息，从而可能导致乒乓缓存现象。
    

为了减轻或避免乒乓缓存现象，可以采取以下策略：

1.  避免共享数据：尽量减少不同线程间的共享数据，让每个线程独立工作在自己的数据集上。
    
2.  使用缓存对齐和填充：通过对齐数据结构，确保不同线程访问的数据位于不同的缓存行中。填充可以用来增加数据结构之间的间隔，以避免它们位于同一个缓存行内。
    
3.  使用线程局部存储：为每个线程分配独立的数据结构，从而避免共享数据引发的缓存竞争。
    
4.  使用原子操作：通过使用原子操作（如 std::atomic）来减少锁的使用，从而降低缓存竞争的可能性。
    

总之，乒乓缓存现象是多核处理器中的一种性能问题，主要由于多个线程或核心频繁地访问共享数据而引发。通过采取一些策略，如减少共享数据、使用缓存对齐和填充、使用线程局部存储和原子操作等，可以有效地减轻或避免乒乓缓存现象，从而提高多线程程序在多核处理器上的性能。

5.  优化任务划分和任务调度：合理地划分任务并分配给各个线程，可以减少它们之间的数据交互，从而降低乒乓缓存现象的影响。此外，合理地调度线程以减少跨核心的调度，也有助于减少缓存行在处理器核心之间的传递。
    
6.  利用 NUMA 架构：在非统一内存访问（NUMA）架构中，处理器被划分为多个 NUMA 节点，每个节点具有自己的内存。将线程与其访问的数据放置在同一个 NUMA 节点上，可以减少跨节点的数据传输，从而降低乒乓缓存现象的影响。
    
7.  使用休眠技术：当线程在等待数据时，可以让其进入休眠状态，从而降低竞争的强度。这样可以减轻乒乓缓存现象，但可能会导致延迟的增加。
    
8.  减少锁粒度：使用细粒度锁可以将锁定范围缩小到仅涵盖可能发生竞争的代码段，从而降低缓存竞争的可能性。然而，过度使用细粒度锁可能导致锁的管理开销过大。
    
9.  采用锁自旋技术：锁自旋技术是一种锁定策略，当线程无法获取锁时，它会一直尝试获取锁，而不是立即挂起。在某些场景中，这可以减少乒乓缓存现象，但在其他场景中可能导致资源浪费。
    

通过这些策略，开发人员可以在多线程程序中降低乒乓缓存现象的影响，提高在多核处理器上的性能。需要注意的是，这些策略需要根据具体的应用场景和硬件环境来权衡选择，以达到最佳的性能提升效果。

## 优化数据结构以提升多线程性能

1.  选择适当的数据结构：选择适合多线程并行计算的数据结构是非常重要的。例如，有些数据结构（如链表、队列和栈）可以更容易地实现无锁编程，从而提高多线程并行计算的性能。
    
2.  避免伪共享：伪共享是指多个线程访问同一个缓存行中的不同数据，导致缓存行无法高效地在处理器间共享。要避免伪共享，可以使用缓存行对齐的数据结构，以确保一个缓存行中只包含一个线程所需的数据。
    
3.  线程局部存储：为每个线程分配独立的数据存储空间，可以减少不同线程之间的数据竞争。这种方法可以提高多线程并行计算的性能，但可能会增加内存开销。
    
4.  分区数据：将共享数据划分为多个独立的区域，每个线程在计算过程中只操作自己负责的数据区域。这种方法可以减少线程之间的数据竞争，从而提高多线程并行计算的性能。
    
5.  使用无锁数据结构：无锁数据结构是一种使用原子操作而不是互斥锁来管理共享数据的数据结构。无锁数据结构可以降低多线程并行计算中的同步开销，从而提高性能。常见的无锁数据结构包括无锁队列、无锁栈和无锁哈希表等。
    
6.  采用并行算法：在设计数据结构时，可以采用并行算法来进一步提高多线程并行计算的性能。例如，可以使用并行排序算法、并行搜索算法和并行聚合算法等。
    
7.  优化任务划分和任务调度：合理地划分任务并分配给各个线程，可以减少它们之间的数据交互，从而降低竞争和同步开销。此外，合理地调度线程以减少跨核心的调度，也有助于提高性能。
    
8.  使用 NUMA 架构：在非统一内存访问（NUMA）架构中，处理器被划分为多个 NUMA 节点，每个节点具有自己的内存。将线程与其访问的数据放置在同一个 NUMA 节点上，可以减少跨节点的数据传输，从而降低同步开销。
    

## 一个异常安全的std::accumulate

```cpp
#include <future>
#include <numeric>

template <typename Iterator, typename T>
T parallel_accumulate(Iterator first, Iterator last, T init) {
  std::size_t len = std::distance(first, last);
  std::size_t max_chunk_size = 25;
  if (len <= max_chunk_size) {
    return std::accumulate(first, last, init);
  }
  Iterator mid_point = first;
  std::advance(mid_point, len / 2);
  std::future<T> l =
      std::async(parallel_accumulate<Iterator, T>, first, mid_point, init);
  // 递归调用如果抛出异常，std::async 创建的 std::future 将在异常传播时被析构
  T r = parallel_accumulate(mid_point, last, T{});
  // 如果异步任务抛出异常，get 就会捕获异常并重新抛出
  return l.get() + r;
}
```

## 并发以提高效率
1. 用多线程隐藏延迟
2. 用并发提高响应度
gui程序往往很看重响应度，单线程程序很难做到高响应
```cpp
std::thread task_thread;
std::atomic<bool> task_cancelled(false);

void gui_thread() {
  while (true) {
    event_data event = get_event();
    if (event.type == quit) {
      break;
    }
    process(event);
  }
}

void task() {
  while (!task_complete() && !task_cancelled) do_next_operation();
  if (task_cancelled) {
    perform_cleanup();
  } else {
    post_gui_event(task_complete);
  }
}

void process(const event_data& event) {
  switch (event.type) {
    case start_task:
      task_cancelled = false;
      task_thread = std::thread(task);
      break;
    case stop_task:
      task_cancelled = true;
      task_thread.join();
      break;
    case task_complete:
      task_thread.join();
      display_results();
      break;
    default:
      ...
  }
}
```

并行版for_each
```cpp
#include <algorithm>
#include <future>

template <typename Iterator, typename Func>
void parallel_for_each(Iterator first, Iterator last, Func f) {
  std::size_t len = std::distance(first, last);
  if (!len) {
    return;
  }
  std::size_t min_per_thread = 25;
  if (len < 2 * min_per_thread) {
    std::for_each(first, last, f);
    return;
  }
  const Iterator mid_point = first + len / 2;
  std::future<void> l =
      std::async(&parallel_for_each<Iterator, Func>, first, mid_point, f);
  parallel_for_each(mid_point, last, f);
  l.get();
}
```

### 并行版 [std::find](https://en.cppreference.com/w/cpp/algorithm/find)

-   [std::find](https://en.cppreference.com/w/cpp/algorithm/find) 的不同之处在于，只要找到目标值就应该停止继续查找。在并行版本中，一个线程找到了值，不仅自身要停止继续查找，还应该通知其他线程停止，这点可以使用一个原子变量作为标记来实现
-   有两种可选方式来返回值和传播异常，一是使用 [std::future](https://en.cppreference.com/w/cpp/thread/future) 数组和 [std::packaged_task](https://en.cppreference.com/w/cpp/thread/packaged_task) 将返回值和异常交给主线程处理，二是使用 [std::promise](https://en.cppreference.com/w/cpp/thread/promise) 直接设置最终结果。如果想在首个异常上终止（即使没有处理完所有元素）则使用 [std::promise](https://en.cppreference.com/w/cpp/thread/promise)，如果想让其他线程继续搜索则使用 [std::packaged_task](https://en.cppreference.com/w/cpp/thread/packaged_task) 保存所有异常，并在没有找到目标值时重新抛出其中一个异常。这里选择使用行为更接近 [std::find](https://en.cppreference.com/w/cpp/algorithm/find) 的 [std::promise](https://en.cppreference.com/w/cpp/thread/promise)

```cpp
#include <atomic>
#include <future>

template <typename Iterator, typename T>
Iterator parallel_find_impl(Iterator first, Iterator last, T match,
                            std::atomic<bool>& done_flag) {
  try {
    std::size_t len = std::distance(first, last);
    std::size_t min_per_thread = 25;
    if (len < (2 * min_per_thread)) {
      for (; first != last && !done_flag.load(); ++first) {
        if (*first == match) {
          done_flag = true;
          return first;
        }
      }
      return last;
    }
    const Iterator mid_point = first + len / 2;
    std::future<Iterator> async_res =
        std::async(&parallel_find_impl<Iterator, T>, mid_point, last, match,
                   std::ref(done_flag));
    const Iterator direct_res =
        parallel_find_impl(first, mid_point, match, done_flag);
    return direct_res == mid_point ? async_res.get() : direct_res;
  } catch (...) {
    done_flag = true;
    throw;
  }
}

template <typename Iterator, typename T>
Iterator parallel_find(Iterator first, Iterator last, T match) {
  std::atomic<bool> done_flag(false);
  return parallel_find_impl(first, last, match, done_flag);
}
```


